{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53. Ch 05. Text Classification - 10. 실습 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- python train.py\n",
    "\n",
    "- python train.py --model_fn ./models/review.pth --train_fn ./data/review.sorted.uniq.refined.tok.shuf.train.tsv --gpu_id -1 --batch_size 128 --n_epochs 10 --word_vec_size 256 --dropout .3 --rnn --hidden_size 512 --n_layers 4 --cnn --window_sizes 3 4 5 6 7 8 --n_filters 128 128 128 128 128 128\n",
    "\n",
    "- head -n 1 ./data/review.sorted.uniq.refined.tok.shuf.test.tsv\n",
    "\n",
    "- cut -f2 ./data/review.sorted.uniq.refined.tok.shuf.test.tsv | shuf | head -n 10 | python ./classify.py --model_fn ./models/review.pth --gpu_id -1\n",
    "\n",
    "- echo “배송이 늦게 왔지만 제품 자체는 정말 좋네요.” | mecab -0 wakati | python ./classify.py —model_fn ./models/review.pth —gpu_id -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54. Ch 05. Text Classification - 11. 정리하며"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<확률적 관점에서>\n",
    "문장이 주어졌을 때, 문장이 속할 클래스의 확률분포함수를 모사\n",
    "\n",
    "<결론>\n",
    "-신경망은 텍스트를 입력으로 받아 context vector로 인코딩\n",
    "RNN의 경우 단어의 출현 여부와 순서에 따른 정보를 종합적으로 활용\n",
    "CNN의 경우 문구의 출현 여부를 종합적으로 활용\n",
    "\n",
    "-Context vector는 y(여기서 y는 class)를 예측하기 위한 정보를 담고 있을 것\n",
    "\n",
    "-자연어 생성 미리보기:\n",
    "시퀀스투시퀀스 인코더의 경우 문장을 생성하기 위한 context vector 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55. Ch 05. Text Classification - 12. Appendix Text Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "최근에는 빅언어모델을 만들어서 프리트레이닝하고 특정 다운스트림태스크(예.텍스트 분류)에 대해 파인튜닝하는 것이 대세.\n",
    "버트를 사용해 성능 몇프로 올리는 데 들어가는 비용을 잘 생각해서 사용해야 한다.\n",
    " \n",
    "<What is BERT?>\n",
    "-Take Encoder form Transformer\n",
    "트랜스포머에서 인코더 부분을 떼온 것이라고 할 수 있다.\n",
    "인코더를 가지고 큰 언어모델 학습하고 거기에 대해서 파인 튜닝(프리트레이닝)하겠다.\n",
    "프리트레이닝은 인터넷에 무궁무진한 문장을 가지고 학습하겠다는 것.\n",
    "문장의 특징을 배우고 그다음에 하고 싶은 다운스트림 태스크에 파인튜닝\n",
    "\n",
    "-Masked Language Models(MLM)\n",
    "Denoising Autoencoders\n",
    "마스크된 문장을 오토인코더에 인풋으로 넣고\n",
    "디노이징 하는 과정에서 문장의 특징을 배우게 됨.\n",
    "-Next Sentence Prediction(NSP)\n",
    "\n",
    "<Transfer Learning via BERT>\n",
    "-Bi-directional Sequential Modeling(using Transformer)\n",
    "-Pretraining using MLM and NSP, and fine-tuning to downstream tasks.\n",
    "예. 텍스트 분류, MRC\n",
    "\n",
    "Big Dataset - pretraining -> BERT - Load weights -> Model <- Fine-tuning - Target Dataset\n",
    "\n",
    "<Where to use>\n",
    "-SKTBrain KoBERT\n",
    "-Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. Ch 05. Text Classification - 13. Appendix Text Classification with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-패스트텍스트를 활용해 굉장히 간단하게 텍스트 분류할 수 있다.\n",
    "인풋 데이터 넣어줄 때 레이블 데이터에 __label__ 붙여줘야 한다.\n",
    "Find: ^(positive|negative)\\t\n",
    "Replace: __label__$1  # $1은 첫번째 괄호를 뜻함\n",
    "시간이 없을 때 활용해보는 것도 좋음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 57. Ch 06. Summary - 01. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1)Introduction to NLP\n",
    "2)Preprocessing\n",
    "3)Word Embedding\n",
    "4)Text Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
