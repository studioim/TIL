{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41. Ch 04. Word Embedding - 15. 타 분야 적용 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<자연어처리 분야>\n",
    "-딥러닝의 모토는 엔드투엔드 솔루션을 만드는 것\n",
    "-단어 임베딩이 최종 목표인 경우는 거의 없음\n",
    "\t-따라서 워드 임베딩 벡터를 활용해 제품을 만드는 일은 흔치 않다\n",
    "\t-보통 텍스트분류한다든가 챗봇을 만든다든가 한다.\n",
    "-워드투벡의 비지도학습 특성을 활용하려는 시도는 있으나, 상용화하기엔 부족\n",
    "\n",
    "<추천 시스템: 상품 임베딩(Product2Vec)>\n",
    "-상품의 특징:\n",
    "\t-Discrete & Sparse\n",
    "\t-클릭/구매 순서에 따른 상품들의 시퀀스 생성 가능\n",
    "-가정: 사용자가 함께 클릭/구매한 상품들이 비슷한 상품들은 비슷한 임베딩 값을 가진다.\n",
    "-예시: 상품의 ID 자체가 단어가 되는 것!\n",
    "\t-258437, 572837, 567288, (684592), 547283, 295729, 692913\n",
    "\t-상품의 이름을 워드투벡으로 돌려서 어떻게 하겠다는 게 아니라\n",
    "\t-침대>벙커 침대, 소파>빈백\n",
    "\n",
    "<주식 종목 임베딩(Stock2Vec)>\n",
    "-종목의 특징\n",
    "\t-Discrete & Sparse\n",
    "\t-시퀀스 뭐라도 만들어보자!(예. 사용자가 매수/매도한 시퀀스)\n",
    "\t-P(오름|주식, 히스토리)\n",
    "-가정: 개장일에 함께 오르고 내린 종목들은 비슷한 임베딩 값을 갖는다.\n",
    "-예시: 상한가부터 하한가까지 쭉 줄 세워보자\n",
    "\t-016710(대성홀딩스) 009450(경동나비엔) 001440(대한전선) 005380(현대자동차) 000640(동아쏘시오홀딩스) 003470(유안타증권) 011760(현대상사)\n",
    "\t-이러한 데이터를 스킵그램 같은 것으로 분석, 10년 쌓인 데이터라면?\n",
    "\n",
    "워드임베딩 벡터는 자연어처리 분야에서 잘 쓰이지는 않지만 굉장히 활용할 부분이 많다.\n",
    "단어와 비슷한 속성을 가진 데이터라면 얼마든지 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42. Ch 04. Word Embedding - 16. Appendix - Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<센텐스 임베딩 via 워드 임베딩>\n",
    "-흔히 시도되는 방법은 문장 내 단어 임베딩 벡터들을 모두 더하는 것\n",
    "\t-완전 안 되는 것은 아니다\n",
    "-하지만 어순을 무시한 채 평균을 구하는 것이므로 디테일이 뭉개질 수 있다.\n",
    "\t-I did not say that i am idiot.\n",
    "\t-I did say that I am not idiot.\n",
    "분류 군집할 때 단순 합은 좋은 방법이 아닐 수 있다.\n",
    "\n",
    "<컨텍스트 임베딩>\n",
    "-단어는 문맥(또는 문장 내 위치)에 따라서 의미가 변화\n",
    "\t-따라서 문맥에 따른 임베딩이 필요함\n",
    "-문맥을 고려하기 위해서는 주변 단어들의 쓰임새를 살펴봐야 함\n",
    "\t-이후 다룰 <텍스트 분류>(RNN), <자연어 생성>(Seq2seq) 주제들의 기본 원리\n",
    "따로 센텐스 임베딩을 위해 워드 임베딩벡터를 구해, 더해서 사용하는 것보다\n",
    "텍스트분류나 자연어 생성에 대한 딥뉴럴 네트워크 동작 원리를 자세히 이해할 필요가 있다.\n",
    "이것을 활용해서 우리가 원하는 형태로 문장이 주어졌을 때 이를 임베딩하는 뉴럴 네트워크를 만들어내면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43. Ch 04. Word Embedding - 17. 정리하며"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Word Sense>\n",
    "-이산적discrete(or symbolic)인 겉 형태와 달리 내부는 연속적\n",
    "-계층적 구조를 지니고 있음\n",
    "\t-상위어 하위어 존재\n",
    "-동의어와 동형어 다의어\n",
    "-원핫 인코딩은 이 모든 것과 배치됨\n",
    "\t-유사도를 나타낼 수 없다(서로 직교하는 존재)\n",
    "\t-상위어 하위어 나타낼 수 없음\n",
    "\t-하나의 원핫인코딩이 여러개의 의미를 나타낸다는 것도 효과적으로 표현할 수 없음\n",
    "\t-실제로 우리가 쓰는 단어와 다른 표현이다\n",
    "\n",
    "<Previous Methods for Word Embedding>\n",
    "-TF-IDF\n",
    "\t-TF-IDF(w,d) = TF(w,d)/DF(w)\n",
    "-TF-IDF Matrix(or Term-frequency Matrix)\n",
    "\t-각 문서에서의 중요도를 피처로 삼아서 벡터를 만들자\n",
    "-Co-occurrence Matrix by context Windowing\n",
    "\t-윈도 내에서 함께 나타나는 단어들의 빈도를 카운트하자\n",
    "\t-0이 나타날 가능성이 꽤 있다.\n",
    "-> 여전히 Sparse(희소)\n",
    "\n",
    "<워드 임베딩 알고리즘>\n",
    "-Word2Vec(Skip-gram)\n",
    "분류\n",
    "-GloVe\n",
    "회귀\n",
    "Co-occurrence 매트릭스의 row를 회귀하는 태스크\n",
    "훨씬 더 효율적으로 빠르게 계산할 수 있다.\n",
    "-FastText\n",
    "Sum of subword embedding\n",
    "기존의 스킵그램은 단어를 원핫벡터로 나타냈던 데 비해\n",
    "패스트텍스트는 단어를 서브워드로 쪼갠 뒤 서브워드들의 임베딩 벡터를 다 더해서 단어의 임베딩 벡터로 나타냄.\n",
    "\n",
    "<임베딩 레이어>\n",
    "-신경망에는 원핫벡터를 넣는 것이 정석\n",
    "-임베딩 레이어는 계산의 효율성을 위해 존재\n",
    "\t-임베딩 레이어는 단순히 원핫벡터를 넣어주기 위해 존재(어차피 선형이라서 겹쳐봤자 선형)\n",
    "\t-원핫벡터와 임베딩레이어웨이트 간의 내적을 훨씬 더 효율적으로 수행하도록 구현해놨다.\n",
    "워드투벡과 같은 워드임베딩 알고리즘은 자연어처리 실무에서 잘 쓰지 않는다.\n",
    "워드임베딩 벡터가 최종 산출물이 되는 경우는 잘 없음. 유사도 확인 정도. 어떤 프로덕트를 만들기 힘들다.\n",
    "임베딩 레이어를 사용하기 때문에 워드임베딩 알고리즘을 사용할 일은 거의 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : 44. Ch 05. Text Classification - 01. 들어가며"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
