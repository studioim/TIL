{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. Ch 03. Preprocessing - 17. 미니배치 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "미니배치, 토치 텍스트\n",
    "모델에 넣기 위한 마지막 변환 과정\n",
    "Split Train/Valid/Test -> Read text& Build dictionary(index mapping) -> Sort by length\n",
    "-> Conver to integer -> Batchify and pad -> Shuffle and feed\n",
    "\n",
    "<Read text& Build dictionary>\n",
    "-빈도 순으로 단어 사전 정렬\n",
    "-필요에 따라 min_count보다 작은 빈도를 갖는 단어는 제외\n",
    "또는 max_vocab에 따라 빈도순으로 어휘를 제외하기도 함\n",
    "-필요에 따라 특수 토큰도 어휘 사전에 포함\n",
    "<BOS>, <EOS>, <UNK>, <PAD> 등\n",
    "Begin of Sentence, End of Sentence, Unknown Token, Padding\n",
    "\n",
    "나는 뷁뛣에 가서 아침 식사를 했어요.\n",
    "-> 나 는 뷁뛣 에 가 서 아침 식사 를 했 어 요 .\n",
    "-> <BOS> 나 는 <UNK> 에 가 서 아침 식사 를 했 어 요 . <EOS>\n",
    "\n",
    "<Chunking and Padding>\n",
    "-미니배치의 형태\n",
    "(batch_size, length, |V|)\n",
    "|V|: 보카뷰러리 사이즈, 하나가 원 핫 벡터가 됨\n",
    "미니배치가 이러한 형태의 텐서가 된다\n",
    "원핫벡터가 길이 만큼 있어서 문장이 이뤄지고 문장들이 모여서 미니배치가 되는 것\n",
    "\n",
    "-원핫 벡터를 다 저장할 필요가 없음\n",
    "인덱스 값만 가지고 있으면 된다.\n",
    "(batch_size, length, 1) =  (batch_size, length)\n",
    "\n",
    "-Sequence 차원의 크기는 미니배치 내의 가장 긴 문장에 의해 결정됨\n",
    "비어 있는 부분은 뭔가로 채워줘야 한다.\n",
    "각 샘플별 모자라는 부분은 패딩으로 대체 , 따라서 <PAD> 토큰이 필요.\n",
    "패드에 대한 인덱스도 생각해줘야 한다는 것.\n",
    "나중에 파이토치의 PackedSequence를 활용할 경우, <PAD> 생략 가능\n",
    "패드 토큰은 여기에 토큰이 있긴 한데 비어 있는 토큰이다고 표시해놓는 것.\n",
    "\n",
    "-패딩해주면 문제점? 재수없어서 처음에는 100단어 문장, 다음에는 2단어 문장이라면\n",
    "98타임스텝이 패드로 채워진다.\n",
    "-> 길이에 맞는 미니배치 필요\n",
    "\n",
    "<Increase Training Efficiency>\n",
    "1)sort by sequence length\n",
    "2)Get chunks with similar length of sequences.(chunking)\n",
    "청킹 이후 미니배치 순서를 셔플링해주면 된다.\n",
    "훨씬 효율적으로 학습 진행 가능\n",
    "\n",
    "<효율적 학습이 가능한 미니배치 만들기>\n",
    "1)코퍼스의 각 문장들을 길이에 따라 정렬\n",
    "2)각 토큰들을 사전을 활용해 str->index맵핑\n",
    "3)미니배치 크기대로 청킹\n",
    "4)각 미니배치 별 텐서 구성 및 패딩\n",
    "5)학습 시 미니배치 셔플링해 iterative하게 반환\n",
    "\n",
    "이걸 편리하게 해주는 라이브러리가 토치텍스트\n",
    "\n",
    "<Torch Text>\n",
    "앞서 소개한 작업들을 수행해주는 파이토치 공식 텍스트 로딩용 라이브러리\n",
    "현재버전 0.5.1\n",
    "\n",
    "<Define Task: what you want>\n",
    "-f(text) = class\n",
    "Text classification\n",
    "-f(text) = word\n",
    "Language Modeling\n",
    "-f(text) = text\n",
    "Machine Translation\n",
    "\n",
    "텍스트, 클래스의 텐서 모양\n",
    "|text| = (batch_size, length, |V|)\n",
    "|class| = (batch_size, length, |C|)\n",
    "\n",
    "<Step 1: Define Fields> \n",
    "필드 정의\n",
    "\n",
    "self.label = data.Field(sequential=False,\n",
    "\t\t\tuse_vocab=True,\n",
    "\t\t\tunk_token=None\n",
    "\t\t\t)\n",
    "self.text = data.Field(use_vocab=True,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tinclude_lengths=False,\n",
    "\t\t\teos_token=‘<EOS>’ if use_eos else None\n",
    "\n",
    "<Step2: Define Dataset with Fields>\n",
    "train, valid = data.TabularDataset.splits(path=‘’.\n",
    "\t\t\t\t\ttrain=train_fn,\n",
    "\t\t\t\t\tvalidation=valid_fn,\n",
    "\t\t\t\t\tformat=‘tsv’,\n",
    "\t\t\t\t\tfields=[(‘label’, self.label),\n",
    "\t\t\t\t\t\t(‘text’, self.text)\n",
    "\t\t\t\t\t\t]\n",
    "\t\t\t\t\t)\n",
    "\n",
    "<Step3: Get DataLoaders from Datasets>\n",
    "self.train_iter, self.valid_iter = data.BucketIterator.splits((train, valid),\n",
    "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\tdevice=‘cuda:%d’ % device if device >= 0 else ‘cpu’,\n",
    "\t\t\t\t\t\t\tshuffle=shuffle,\n",
    "\t\t\t\t\t\t\tsort_key=lambda x: len(x.text),\n",
    "\t\t\t\t\t\t\tsort_within_batch=True\n",
    "\t\t\t\t\t\t\t)\n",
    "self.label.build_vocab(train)\n",
    "self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)\n",
    "\n",
    "테스트셋 셔플링 안 해준다, 트레이닝, 밸리데이션 셋에만\n",
    "\n",
    "<Example Usage:>\n",
    "-Vocabulary(or Class) size\n",
    "vocab_size = len(dataset.text.vocab)\n",
    "n_classes = len(dataset.label.vocab)\n",
    "print(‘|vocab| =’, vocab_size, ‘|classes| =’, n_classes)\n",
    "\n",
    "-Feeding with Ignite\n",
    "trainer.run(train_loader, max_epochs=self.config.n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Ch 03. Preprocessing - 18. 실습 TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-문서 라인 수\n",
    "wc -l ./review.sorted.uniq.refined.tok.*\n",
    "\n",
    "-셔플링\n",
    "gshuf < review.sorted.uniq.refined.tok.tsv > review.sorted.uniq.refined.tok.shuf.tsv\n",
    "\n",
    "-train 셋\n",
    "head -n 212680 ./review.sorted.uniq.refined.tok.shuf.tsv > review.sorted.uniq.refined.tok.shuf.train.tsv\n",
    "\n",
    "-테스트 셋\n",
    "tail -n 90000 ./review.sorted.uniq.refined.tok.shuf.tsv > review.sorted.uniq.refined.tok.shuf.test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Ch 03. Preprocessing - 19. 정리하며"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<NLP 프로젝트 워크플로>\n",
    "-문제 정의 \n",
    "단계를 나누고 simplify\n",
    "x와 y를 정의\n",
    "-데이터 수집 \n",
    "문제 정의에 따른 수집\n",
    "필요에 따라 레이블링\n",
    "-데이터 전처리 및 분석\n",
    "형태를 가공\n",
    "필요에 따라 EDA 수행\n",
    "-알고리즘 적용\n",
    "가설을 세우고 구현/적용\n",
    "-평가\n",
    "실험 설계\n",
    "테스트셋 구성\n",
    " -배포\n",
    "RESTful API를 통한 배포\n",
    "상황에 따라 유지/보수\n",
    "\n",
    "<전처리 워크플로>\n",
    "-데이터(코퍼스) 수집\n",
    "구입, 외주\n",
    "크롤링을 통한 수집\n",
    "-정제\n",
    "태스크에 따른 노이즈 제거(이 노이즈가 의미가 있을까를 고민)\n",
    "인코딩 변환(utf-8로 통일 추천, 다국어 다루기 위해선)\n",
    "-레이블링(Optional)\n",
    "문장마다 또는 단어마다 레이블링 수행\n",
    "-토크나이제이션\n",
    "형태소 분석기를 활용해 분절 수행\n",
    "한국어는 필수. 교착어이기 때문에. 접사를 분리하지 않으면 희소성이 커진다.\n",
    "-서브워드 세그멘테이션(Optional)\n",
    "단어보다 더 작은 의미 단위\n",
    "추가 분절 수행\n",
    "-Batchify\n",
    "사전 생성 및 word2index 맵핑 수행\n",
    "효율화를 위한 전/후처리\n",
    "torchText를 활용해 쉽게 처리할 수 있다.\n",
    "\n",
    "<Service Pipeline>\n",
    "-정제(regex, encoding)\n",
    "학습데이터와 같은 방식의 정제 수행\n",
    "-토크나이제이션(mecab)\n",
    "학습 데이터와 같은 방식의 분절 수행\n",
    "-서브워드 세그멘테이션(BPE)\n",
    "학습 데이터로부터 얻은 '모델'을 활용해 똑같은 분절 수행\n",
    "코퍼스마다 모델이 새롭게 하나씩 생성(BPE를 학습하는 것이기 때문에)\n",
    "-Batchify(torchtext)\n",
    "학습 데이터로부터 얻은 사전에 따른 word2index 맵핑 수행\n",
    "-Prediction\n",
    "모델에 넣고 추론 수행\n",
    "필요에 따라 serach 수행(자연어 생성)\n",
    "-Detokenization(Optional)\n",
    "사람이 읽을 수 있는 형태로 변환(index2word)\n",
    "분절 복원\n",
    "\n",
    "보통은 API 서버를 만들텐데 이 서버 안에 위의 파이프라인이 다 들어가 있어야 함\n",
    "\n",
    "<요약>\n",
    "-정제\n",
    "태스크와 언어 및 도메인에 따른 특성\n",
    "\t풀고자 하는 문제의 특성에 따라 전처리 전략이 다름\n",
    "끝이 없는 과정\n",
    "\t노력과 품질 사이의 트레이드 오프\n",
    "\tsweet spot을 찾아야 함\n",
    "-분절\n",
    "한국어의 경우 띄어쓰기 노말라이제이션을 위해 형태소 분석기 활용이 필요\n",
    "서브워드 세그멘테이션을 통해 좀 더 잘게 분절할 수 있음\n",
    "-모두 비슷한 알고리즘을 사용하고 있으므로, 결국 데이터의 양과 품질이 좌우함\n",
    "따라서 전처리 과정을 경시해서는 안 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
