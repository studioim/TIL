{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48. Ch 05. Text Classification - 05. CNN을 활용한 텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<CNN>\n",
    "본디 CNN은 컴퓨터 비전 분야에서 활발히 사용\n",
    "3바이3 커널과 1바이1패딩이 만나면 인풋과 아웃풋의 크기가 동일.\n",
    "그래서 맥스풀링(다운 샘플링 기법), 스트라이드로 차원축소\n",
    "\n",
    "<Motivations>\n",
    "-감성분석(setiment analysis)\n",
    "긍정: 좋은 행복 만족 훌륭한\n",
    "부정: 형편 없는, 느린, 비추, 개판 직전\n",
    "-클래스를 결정하는 일부 문구의 패턴이 있기 마련\n",
    "-이때 문구 내의 단어들을 비슷한 의미의 단어들로 치환할 수 있을 것\n",
    "좋은 훌륭한 만족스러운 대박\n",
    "-비슷한 의미의 문구의 임베딩 벡터의 패턴을 인식할 수 있다면?\n",
    "\n",
    "<Introduction>\n",
    "Convolutional Neural Networks for Sentence Classification[Kim, 2014]\n",
    "CNN을 NLP에도 쓸 수 있구나!\n",
    "이 논문을 가지고 구현해볼 것이다.\n",
    "\n",
    "<Overview>\n",
    "n x k representation of sentence  with static and non-static channels\n",
    "-> Convolutional layer with multiple filter widths and feature maps\n",
    "-> Max-over-time pooling\n",
    "-> 컨텍스트 벡터\n",
    "-> Fully connected layer with dropout and softmax output\n",
    "-> Positive or Negative\n",
    "\n",
    "<텍스트 분류 with CNN>\n",
    "m: 문장 길이\n",
    "d: 임베딩 벡터 크기\n",
    "w: 윈도우 크기(패턴 내 단어의 개수)\n",
    "필터 개수와 같은 히트맵 개수가 나오게 된다.\n",
    "하나의 커널은 특정 단어의 패턴을 찾는다.\n",
    "자동으로 커널이 백프로퍼게이션으로 패턴을 찾는다.\n",
    "\n",
    "예를 들어 하나의 필터는 ‘완전 좋아’의 패턴을 찾아 히트맵에 표시\n",
    "다른 필터는 ‘정말 싫어’의 패턴을 찾아 히트맵에 표시\n",
    "\n",
    "두 단어짜리(w=2) 디텍트하는 CNN, 세 단어짜리(w=3) 디텍트하는 CNN이 있을 수 있다\n",
    "윈도우 사이즈 별로 히트맵의 집합이 나올 수 있다.\n",
    "\n",
    "우리가 필요한 것은 문장 내에서 가장 높은 점수 하나(문장 내에서 히트가 됐니 안 됐니, 해당 커널에 대한 피처가 있었니 없었니)\n",
    "맥스풀링을 해서 히트맵에서 가장 큰 거 하나 가져오고, 그 다음 히트맵에서 가장 큰 거 하나 가져오고…\n",
    "그렇게 문장 임베딩 벡터(히든 벡터, 레이턴트 벡터)를 만든다(문장 내의 하이스트 히트 스코어의 모임)\n",
    "\n",
    "문장 길이는 가변, 그런데 맥스풀링을 하는 순간 가변적인 변수가 없어진다.\n",
    "문장 길이에 상관 없이 문장 임베딩 벡터의 길이는 동일\n",
    "\n",
    "문장 임베딩 벡터는 해석이 어려울 수 있다.\n",
    "\n",
    "<요약>\n",
    "-RNN에 비해 좀 더 직관적인 방법(코드, 구조가 더 복잡할 수 있긴 함)\n",
    "-RNN은 문장의 문맥(context)를 이해한다면(고 해석해 볼 수 있음), CNN은 문장 내 단어의 패턴을 인식(해당 문구가 있니 없니)\n",
    "-당시 논문은 워드 임베딩을 직접 수행(프리트레인)한 이후에 넣어주는 것을 추천했지만,\n",
    "임베딩 레이어에 원핫벡터를 넣어줘 구현할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. Ch 05. Text Classification - 06. 실습 CNN 분류기 구현하기\n",
    "## 50. Ch 05. Text Classification - 07. 실습 Trainer 구현하기\n",
    "## 51. Ch 05. Text Classification - 08. 실습 train.py 구현하기\n",
    "## 52. Ch 05. Text Classification - 09. 실습 classify.py 구현하기\n",
    "- Text Classification 폴더 참고"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
