{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Ch 04. Word Embedding - 01. 들어가며"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "왜 워드 임베딩 해야 하나?\n",
    "<word: Discrete, not Continuous>\n",
    "-단어는 discrete symbol & categorical value 형태이지만, 우리의 머릿속에서는 다르게 동작\n",
    "어휘는 계층적 의미 구조를 지니고 있으며\n",
    "이에 따라 단어 사이의 유사성을 지님\n",
    "예) 파랑과 핑크 중에서 빨강에 가까운 단어는 무엇인가?\n",
    "\n",
    "-원핫인코딩으로 표현된 값은 유사도나 모호성을 표현할 수 없다.\n",
    "sparse vector보다 Dense vector로 표현하는 것이 유리\n",
    "\n",
    "<Feature Vectors>\n",
    "-피처(특징)\n",
    "샘플을 잘 설명하는 특징\n",
    "특징을 통해 우리는 특정 샘플을 수치화할 수 있다.\n",
    "예) 몽타주\n",
    "\n",
    "-피처 벡터\n",
    "각 특징들을 모아서 하나의 벡터로 만든 것\n",
    "\n",
    "-단어의 피처 벡터는 무엇이 될까?\n",
    "\n",
    "<Representation Learning via Dimension Reduction>\n",
    "-신경망은 x와 y사이의 관계를 학습하는 과정에서 자연스럽게 x의 피처를 추출하는 방법을 학습함\n",
    "-레이어 중간의 hidden representation은 y의 값을 구하기 위해 x에 필요한 정보를 더 작은 차원에 압축 표현한 것이라 할 수 있음(오토 인코더 참고)\n",
    "\n",
    "<워드 임베딩>\n",
    "-딥러닝 시대에 들어와 신경망의 이러한 특성을 활용해 단어를 연속적인 값으로 표현하고자 하는 시도가 이어짐\n",
    "Word2vec(CBOW, Skip-gram)\n",
    "-이전에 비해 훌륭한 dense vector를 얻을 수 있게 돼 단어의 필요한 특징을 잘 표현할 수 있게 됐음.\n",
    "유사도 등의 연산에 유리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. Ch 04. Word Embedding - 02. Word Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<차 from 국립국어원>\n",
    "#1 마시는 차\n",
    "#2 바퀴 달린 차\n",
    "#3 둘 이상 비교했을 때 나타나는 수준이나 정도\n",
    "#4 어떤 일의 차례나 횟수\n",
    "\n",
    "<Word, Lemma and Sense>\n",
    "word1 - lemma1 - sense1 sense2, lemma2 - sense3 sense4 sense5 sense6 \n",
    "word2 - lemma3 - sense5 sense6 sense7, lemma4 -sense8 sense9\n",
    "\n",
    "단어들이 내부적으로 다양한 의미를 지니고 있고 다른 단어 의미들과 연결돼 있을 수 있다.\n",
    "\n",
    "<Homonym and Polysemy>동형어와 다의어\n",
    "다의어(Polysemy) / 강남역 / 지하철 역사, 역 주변 상권(의미는 다르지만 근본은 같은 곳에서 출발한 단어)\n",
    "예) 다리\n",
    "동형어(Homonym) / 차 / 마시는 차, 달리는 차(겉으로만 같을 뿐 의미는 전혀 다름)\n",
    "\n",
    "중의성 소거 필요\n",
    "We need Word Sense Disambiguation(WSD)\n",
    "RNN 가게 되면 주변 단어를 보고 파악할 수 있음\n",
    "다의어의 경우는 큰 문제는 안 됨 책상다리나 내 다리나 레그라고 표현하면 알아듣는다\n",
    "동형어는 WSD 필수적으로 필요\n",
    "\n",
    "<Synonyms 동의어>\n",
    "두 개 이상의 단어가 같은 의미를 지니는 경우\n",
    "\n",
    "word1 - lemma1 - sense1 sense2, lemma2 - sense3 sense4 sense5 sense6 \n",
    "word2 - lemma3 - sense5 sense6 sense7, lemma4 -sense8 sense9\n",
    "\n",
    "여기서 워드 1, 2는 sense5, sense6 같은 의미를 지님\n",
    "이 경우 워드1과 2는 동의어다라고 말할 것\n",
    "예) home과 place\n",
    "\n",
    "<Synonyms <> Antonyms반의어>\n",
    "\n",
    "<Hypernyms & Hyponyms>상위어, 하위어\n",
    "-단어는 개념적 의미를 지님\n",
    "-따라서 개념을 포괄하는 상위 개념이 존재하며, 계층적 구조를 지님\n",
    "-상위어 / 하위어\n",
    "동물 / 코끼리\n",
    "전화기 / 휴대폰\n",
    "컴퓨터 / 노트북\n",
    "\n",
    "<그러나>\n",
    "-우리는 원핫 인코딩을 통해 단어를 표현함\n",
    "앞서 다룬 내용을 반영할 수 없음\n",
    "-따라서 다른 방법을 찾아야 한다.\n",
    "\n",
    "워드투벡 나오기 전 전통적 NLP 방법 찾아보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. Ch 04. Word Embedding - 03. WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "단어는 동의어 반의어 상위어 하위어 다양한 특성 띄는데\n",
    "우리는 원핫 representation을 통해 표현하기 때문에 그런 특성들을 잘 반영하지 못하고 있다.\n",
    "\n",
    "워드넷은 이걸 잘 반영하기 위한 어휘 사전으로 좋은 공개 소프트웨어\n",
    "\n",
    "<워드넷>\n",
    "-Thesaurus(어휘분류사전, 시소러스)\n",
    "-심리학 교수인 George Armitage Miller 교수 지도 하에 프린스턴 대학에서 1985년부터 만든 프로그램\n",
    "-동의어 집합(sysnet) 또는 상위어(hyperym)나 하위어(Hyponym)에 대한 정보가 특히 잘 구축돼 있는 것이 장점\n",
    "Direted Acyclic Graph(유방향 비순환 그래프)\n",
    "\n",
    "-NLTK 활용 또는 Web에서 다운\n",
    "\n",
    "<한국어 워드넷>\n",
    "KorLex 부산대\n",
    "Korean WordNet(KWN) 카이스트\n",
    "\n",
    "<Hierarchy in WordNet>\n",
    "entity - person  -worker - employee - mailman\n",
    "\t\t- enrollee - student\n",
    "\t\t-preserver - defender - lawman - sheriff, policeman\n",
    "\t\t\t\t\t-fireman\n",
    "\n",
    "이를 통해 단어와 단어 사이의 거리를 구해볼 수 있다.\n",
    "가장 가까운 상위어를 통해 이어진 거리\n",
    "디스턴스가 가까울수록 의미가 가까운 단어\n",
    "유사도 계산도 가능(두 단어 사이의 거리에 -log 취해주면)\n",
    "\n",
    "코퍼스가 없어도 두 단어 사이의 유사도를 구할 수 있게 된 것\n",
    "\n",
    "<요약>\n",
    "-워드넷을 통해\n",
    "단어의 계층적 구조를 파악할 수 있음\n",
    "동의어 집합(synset)을 구할 수 있음\n",
    "단어 사이의 유사도를 계산할 수 있음\n",
    "-> 이 모든 것이 코퍼스 없이 가능!\n",
    "\n",
    "-하지만 다음의 경우\n",
    "어떤 단어에 대한 자료를 만들던 날, 밀러 교수님이 부인과 다투었다면 정확하지 않을 수 있다.\n",
    "특정 도메인(또는 수집된 데이터)에 특화된 수치를 계산하고 싶음(워드넷은 상식 선에서의 데이터베이스)\n",
    "신조어나 사전에 등록되지 않은 단어(지금은 의미가 달라졌을 수도 있다)\n",
    "-> Data-driven 방식의 필요성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. Ch 04. Word Embedding - 04. 실습 WordNet을 활용한 단어 유사도 계산\n",
    "-주피터 노트북 파일 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. Ch 04. Word Embedding - 05. 딥러닝 이전의 단어 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Data-driven Methods>\n",
    "-Thesaurus 기반 방식은 사전에 대해 의존도가 높으므로, 활용도가 떨어질 수 있음\n",
    "신조어가 들어가 있지 않거나\n",
    "-데이터에 기반한 방식은(데이터가 충분하다면) 태스크에 특화된 활용이 가능\n",
    "도메인에 특화된 활용이 가능하다는 것\n",
    "단 수집된 코퍼스 자체가 해당 도메인에서 수집된 것이어야 함\n",
    "\n",
    "전통적인 방식으로도 데이터 드리븐된 임베딩을 구할 수 있다.\n",
    "\n",
    "<TF_IDF>\n",
    "-텍스트 마이닝에서 중요하게 사용\n",
    "-어떤 단어 w가 문서 d 내에서 얼마나 중요한지 나타내는 수치\n",
    "-TF(Term Frequency)\n",
    "단어의 문서 내에 출현한 횟수\n",
    "숫자가 클수록 문서 내에서 중요한 단어\n",
    "하지만 the와 같은 단어도 TF값이 매우 클 것\n",
    "-IDF(Inverse Document Frequency)\n",
    "그 단어가 출현한 문서의 숫자의 역수(inverse)\n",
    "값이 클수록 the와 같이 일반적으로 많이 쓰이는 단어\n",
    " \n",
    "TF-IDF(w,d) = TF(w,d) / DF(w)\n",
    " \n",
    "<TF-IDF를 피처로 사용할 수 없을까?>\n",
    "-TF-IDF는 문서에서 해당 단어가 얼마나 중요한지 수치화\n",
    "-중요한 문서가 비슷한 단어들은 비슷한 의미를 지닐까?\n",
    "-각 문서에서의 중요도를 피처로 삼아서 벡터를 만든다면?\n",
    "\n",
    "<TF-IDF Matrix>\n",
    "-단어의 각 문서(문장, 주제)별 TF-IDF 수치를 벡터화\n",
    "Row: 단어\n",
    "column : 문서\n",
    "\n",
    "-예제 각 단어별 주제에 대한 TF-IDF 수치\n",
    "단어 / 정치 / 경제 / 사회 / 생활 / 세계 / 연예 /스포츠\n",
    "문재인/ 높음 / 높음 / 높음 / 낮음 / 중간 / 낮음 / 낮음\n",
    "BTS / 낮음 / 낮음 / 낮음 / 낮음 / 높음 / 높음 / 낮음\n",
    "류현진 / 낮음 / 낮음 / 낮음 / 낮음 / 높음 / 중간 / 높음\n",
    "문재인과 BTS 유사도보다 BTS와 류현진의 유사도가 높구나는 것을 알 수 있다.\n",
    "\n",
    "<Based on Context Window(Co-occurrence)>\n",
    "-함께 나타나는 단어들을 활용\n",
    "\n",
    "-가정:\n",
    "의미가 비슷한 단어라면 쓰임새가 비슷할 것\n",
    "쓰임새가 비슷하기 때문에, 비슷한 문장 안에서 비슷한 역할로 사용될 것\n",
    "따라서 함께 나타나는 단어들이 유사할 것\n",
    "\n",
    "-Context window를 사용해 windowing을 실행\n",
    "window의 크기라는 하이퍼 파라미터 추가\n",
    "적절한 윈도우 크기를 정하는 것이 중요\n",
    "\n",
    "-예\n",
    "각 단어별 context window 내에 함께 나타난 빈도\n",
    "단어    / 문재인 / 박근혜 / 이명박 / BTS / 싸이 / 방탄 / 주식 / KOSPI / 양적 완화\n",
    "문재인 /           /   높음   /   높음  / 낮음 / 낮음 / 낮음 / 높음 /   높음   / 낮음\n",
    "박근혜 / 높음   /             /   높음  / 낮음 / 중간 / 낮음 / 높음 /   높음   / 낮음\n",
    "-> 주요 단어만 피처로 활용하는 것도 한 방법\n",
    "\n",
    "<요약>\n",
    "전통적 방식의 워드 임베딩 배워봤다\n",
    "-시소러스 기반 방식에 비해 코퍼스(or 도메인) 특화된 표현 가능\n",
    "시소러스 방식과 데이터 기반 방식의 차이\n",
    "-여전히 sparse한 벡터로 표현됨(문서에 등장하지 않거나 같이 나오지 않으면 0이 되니까)\n",
    "PCA를 통해 차원 축소를 하는 것도 한 방법\n",
    "\n",
    "hand crafted feature 한계를 가질 수도 있다.\n",
    "컬럼 구분을 우리가 직접 해주다보니."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32. Ch 04. Word Embedding - 06. 단어간 유사도(거리) 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similarity Metrics\n",
    "벡터간 거리 구하는 방법\n",
    "<Manhattan Distance(L1 distance)>맨하탄 거리\n",
    "dL1(w,v) = 시그마(i는 1부터 d까지) |wi - vi|, where w, v는 d 차원에 속하는 실수\n",
    "\n",
    "<Euclidean Distance (L2 distance)>유클리드 거리\n",
    "dL2(w,v) =  루트(시그마(i는 1부터 d까지) |wi - vi|제곱), where w, v는 d 차원에 속하는 실수\n",
    "\n",
    "<Infinity Norm>\n",
    "d무한대(w,v) = max(|w1-v1|, |w2-v2|, …, |wd-vd|), where w, v는 d차원 실수에 속한다\n",
    "각 차원에서 가장 큰 거리\n",
    "\n",
    "<L1, L2, Infinity>\n",
    "L1 Norm 내부 그린 사각형\n",
    "L2 Norm 블루 원\n",
    "Infinity Norm 외부 레드 사각형\n",
    "\n",
    "L2의 경우 중심점에서 모든 영역까지의 거리가 모두 같다고 하면 이상하지 않지만\n",
    "L1과 인피니티 거리의 경우에는 이상함\n",
    "우리가 평상시에 알고 있는 거리의 개념은 모르는 사이 유클리드 거리를 정의하고 있었다는 것\n",
    "\n",
    "<cosine similarity>\n",
    "두 벡터간의 거리와 방향까지 보는 유사도\n",
    "simcos(w,v) = wv(내적) / |w||v|, where w, v는 d차원 실수에 속한다\n",
    "서로간의 곱을 모두 더한 값을, 각각의 값을 제곱합한 뒤 제곱근하고 서로 곱한 값으로 나눈 값\n",
    "직교하면 코사인 유사도는 0 완벽하게 방향 일치하면 1\n",
    "벡터간의 방향을 중요하게 생각한다\n",
    "\n",
    "<요약>\n",
    "-L1, L2 놈과 인피니티 놈은 강조하고자 하는 것에 따라 사용\n",
    "L1은 하나만 커지면 안 되고 전체 다 커져야 함\n",
    "인피니티 놈은 하나만 커져도 됨\n",
    "L2는 그 중간\n",
    "\n",
    "-코사인 유사도는 벡터의 방향을 중요시함\n",
    "피처 벡터의 각 차원의 상대적 크기가 중요할 때 사용\n",
    "유사도는 디스턴스의 반대라고 생각하면 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33. Ch 04. Word Embedding - 07. 실습 딥러닝 이전의 단어 임베딩 구현하기\n",
    "-주피터 노트북 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 34. Ch 04. Word Embedding - 08. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Mikolov et al.2013]\n",
    "-목표:\n",
    "주변(context window)에 같은 단어가 나타나는 단어일 수록 비슷한 벡터 값을 가져야 한다.\n",
    "-문장의 문맥에 따라 정해지는 것이 아님\n",
    "방탄은 bts든 bulletproof든 둘 중 하나의 임베딩 벡터가 된다. 중의성 문제를 야기하는 단서가 될 수 있음\n",
    "context window의 사이즈에 따라 임베딩의 성격이 바뀔 수 있다.\n",
    "\n",
    "-CBOW, Skip-gram(더 좋음 더 많이 쓰임) 두 가지 방법\n",
    "CBOW: 주변 단어들이 주어졌을 때 현재 단어 예측\n",
    "Skip-gram: 현재 단어 주어지면 주변 단어 예측\n",
    "\n",
    "<스킵그램: 베이직 컨셉>\n",
    "-기본 전략\n",
    "주변 단어를 예측하도록 하는 과정에서 적절한 단어의 임베딩(정보의 압축)을 할 수 있다.\n",
    "Non-linear activation func이 없음(두 개의 리니어 레이어가 있다)\n",
    "-기본적인 개념은 오토인코더와 굉장히 비슷함\n",
    "y를 성공적으로 예측하기 위해 필요한 정보를 선택/압축\n",
    "x(원핫벡터) -> Encoder -> z(x의 덴스 벡터) -> Decoder -> y hat\n",
    "\n",
    "입력으로는 원핫벡터\n",
    "-> 리니어 레이어에 들어간다(nn.Linear)\n",
    "-> 정해진 차원의 워드 임베딩 벡터(트래디셔널한 방식보다 훨씬 덴스한 벡터)\n",
    "-> 리니어 레이어\n",
    "-> 소프트맥스\n",
    "-> multinoulli Distribution (분류!)\n",
    "\n",
    "-장점(at that time):\n",
    "쉽다\n",
    "빠르다\n",
    "비교적 정확한 벡터를 구할 수 있다\n",
    "-단점(currently):\n",
    "헌데 느리다\n",
    "출현 빈도가 적은 단어일 경우 벡터가 정확하지 않다\n",
    "\n",
    "예전 장점이 단점으로…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 35. Ch 04. Word Embedding - 09. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-Global Vectors for Word Representation\n",
    "[Pennington et al., 2014]\n",
    "-단어x와 윈도우 내에 함께 출현한 단어들의 출현 빈도를 맞추도록 훈련(regression 태스크다)\n",
    "\n",
    "V차원의 원핫벡터 -> 인코더 -> d차원의 워드 임베딩 벡터 -> 디코더 -> 각 단어별 빈도수에 해당하는 V차원의 벡터를 회귀 수행\n",
    "\n",
    "-출현 빈도가 적은 단어에 대해서는 loss의 기여도를 낮춤\n",
    "따라서 출현 빈도가 적은 단어에 대해 부정확해지는 단점을 보완\n",
    "\n",
    "-장점:\n",
    "더 빠르다(스킵그램에 비해 훨씬 빠르다)\n",
    "\t스킵그램은 윈도우를 하나씩 조정해가며 훑지만(이를 n에포크 수행), 글로브는 콘텍스트 윈도우 기반으로 한번에 싹 훑고 회귀 진행\n",
    "\t전체 코퍼스에 대해 각 단어별 co-occurrence(동시 발생)를 구한 뒤, 회귀를 수행\n",
    "출현 빈도가 적은 단어도 벡터를 비교적 정확하게 잘 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 36. Ch 04. Word Embedding - 10. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<스킵그램의 업그레이드 버전>\n",
    "-Enriching Word Vectors with Subword Information\n",
    "[Bojanowski and Grave et al., 2016]\n",
    "\n",
    "-Motivation:\n",
    "기존의 word2vec은 저빈도 단어에 대한 학습과 OoV에 대한 대처가 어려웠음\n",
    "\n",
    "-FastText는 학습 시\n",
    "1)단어를 서브워드로 나누고(저빈도나 Oov 대처 가능, 쪼개 보면 아닐 수 있다)\n",
    "2)스킵그램을 활용해 각 서브워드에 대한 임베딩 벡터에 주변 단어의 컨텍스트 벡터를 곱해 더한다\n",
    "3)이 값이 최대가 되도록 학습을 수행한다.\n",
    "\n",
    "-최종적으로 각 서브워드에 대한 임베딩 벡터의 합이 워드 임베딩 벡터가 된다.\n",
    "단어가 특정 서브워드로 구성돼 있을 때 주변에 단어는 어떻게 나타날까를 구하는 것.\n",
    "\n",
    "<Example: where>\n",
    "“where” = {“<wh”, “whe”, “her”, “ere”, “re>”}\n",
    "더 이상 원핫벡터(희소벡터)는 아님\n",
    "-> linear layer (압축)-> 워드 임베딩 벡터 -> 리니어 레이어(복원) -> 소프트맥스 -> multinoulli Distribution(주변 단어 예측)\n",
    "\n",
    "<결론: 워드 임베딩>\n",
    "Word2vec(스킵그램, CBOW) - Glove - FastText\n",
    "fasttext가 가장 늦게 나왔으니 사용해야 한다? 그것도 맞는 말\n",
    "-딱히 어떤 알고리즘이 더 뛰어나다고는 할 수 없다.\n",
    "구현이 쉽고 빠른 오픈소스를 사용하는 것이 낫다\n",
    "원하는 방향으로 사용\n",
    "\n",
    "-두 개의 다른 알고리즘 결과물을 concat해 하나의 큰 워드 임베딩 벡터로 사용하기도\n",
    "skip-gram + GloVe\n",
    "\n",
    "<Open-source>\n",
    "-Gensim: 잘 돼 있긴 한데 속도가 그렇게 빠르지 않음\n",
    "-GloVe\n",
    "-FastText: 페북 공개, 주로 많이 쓴다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
