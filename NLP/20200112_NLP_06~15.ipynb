{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Ch 02. Introduction - 05. 왜 한국어 자연어처리는 더 어려운가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<교착어>\n",
    "교착어: 한국어 일본어 몽골어 - 어간에 접사가 붙어 단어를 이루고 의미와 문법적 기능이 정해짐\n",
    "굴절어: 라틴어, 독일어, 러시아어 - 단어의 형태가 변함으로써 문법적 기능이 정해짐\n",
    "고립어: 영어, 중국어 - 어순에 따라 단어의 문법적 기능이 정해짐\n",
    "-> 언어의 특성에 따라 전처리 방법이 달라진다\n",
    "\n",
    "<교착어: 접사 추가에 따른 의미 파생>\n",
    "잡 + 다 : 잡다\n",
    "잡 + 히 + 다 : 잡히다\n",
    "잡 + 히 + 시 + 다 : 잡히시다\n",
    "잡 + 히 + 시 + 었 + 다: 잡히셨다.\n",
    "잡히시었겠더라\n",
    "-> 접사의 추가로 단어가 다양하게 파생, 단어 숫자가 엄청나게 늘어난다. 교착어의 특성으로 인해 분절이 필요. 형태소 분석기로 분절하는 작업을 진행해준다.\n",
    "\n",
    "<교착어 : 유연한 단어 순서 규칙>\n",
    "나는 밥을 먹으러 간다\n",
    "간다 나는 밥을 먹으러\n",
    "먹으러 간다 나는 밥을\n",
    "밥을 먹으러 간다 나는\n",
    "…16케이스 중 12케이스가 정상 판정\n",
    "반면 고립어는 순서가 정해져 있다.\n",
    "\n",
    "<모호한 띄어쓰기>\n",
    "- 근대 이전까지 동양권 언어에는 띄어쓰기가 존재하지 않았음.\n",
    "서양에서는 중세시대에 띄어쓰기가 확립됨\n",
    "따라서 아직 우리나라 말은 여전히 띄어쓰기와 궁합을 맞추는 중\n",
    "전 국립언어원장님도 어려워하시는 띄어쓰기\n",
    "왜? 띄어쓰기가 어지간히 틀려도 잘 알아듣기 때문\n",
    "-> 데이터마다 띄어쓰기 중구난방. 전처리에서 띄어쓰기를 normalization 해줄 필요가 있다.\n",
    "\n",
    "<평서문과 의문문의 차이 부재, 주어 부재>\n",
    "언어 / 평서문 / 의문문\n",
    "영어 / I ate my lunch. /  Did you have lunch?\n",
    "한국어 / 점심 먹었어 /  점심 먹었어?\n",
    "\n",
    "<한자 기반의 언어>\n",
    "표의 문자인 한자를 표음 문자인 한글로 Wrapping함\n",
    "표의 문자 : 의미 또는 사물의 형상을 글씨로 나타냄\n",
    "표음 문자: 사람이 말하는 소리, 음성을 글씨로 나타냄\n",
    "wrapping 과정에서 정보의 손실 발생\n",
    "\n",
    "<단어 중의성으로 인한 문제 발생 사례>\n",
    "‘차’의 hidden representation\n",
    "임베딩 시 코카콜라 - 커피 - 차 - 버스 - 오토바이\n",
    "\n",
    "<극악 난이도 한국어 NLP>\n",
    "한글은 굉장히 늦게 만들어진 문자\n",
    "따라서 기존 다른 문자들의 장점을 흡수\n",
    "굉장히 과학적으로 만들어짐\n",
    "효율이 극대화됐기 때문에 더욱 어려운 것\n",
    "앞으로 우리는 자연어처리 전반 뿐만 아니라, 한국어에 적용했을 때의 특성과 문제도 다룰 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Ch 02. Introduction - 06. 딥러닝 자연어처리 주제 및 역사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<overview>\n",
    "Before 딥러닝 - before 시퀀스 투 시퀀스 - after 시퀀스 투 시퀀스 위드 어텐션 - 어텐션 시대 - 프리트레이닝 앤 파인 튜닝\n",
    "\n",
    "<Before 딥러닝>\n",
    "- 전형적인 NLP 애플리케이션의 구조\n",
    "여러 단계의 sub-module로 구성돼 복잡한 디자인을 구성\n",
    "매우 무겁고 복잡해 구현 및 시스템 구성이 어려운 단점\n",
    "각기 발생한 에러가 중첩 및 가중돼 error propagation\n",
    "-> 심볼릭한 데이터를 다루면서 생기는 한계들… 성능 하락으로 이어질 수밖에 없었음\n",
    "\n",
    "<before 시퀀스 투 시퀀스>\n",
    "- 워드 임베딩 (Mikolov et al., 2013)\n",
    "워드투벡(CBOW /  Skip-gram)\n",
    "Text Classification(Kim, 2014)\n",
    "-> Text to Numeric values\n",
    "문장이 주어지면 하나의 벡터로 바꾸는 데 불과했다.\n",
    "\n",
    "<after 시퀀스 투 시퀀스 위드 어텐션>\n",
    "- beyond ‘text to numeric’\n",
    "텍스트를 받아서 텍스트로 만들어주는 시대가 열림\n",
    "아직 어떤 문장을 만들기 위해 어떤 벡터가 필요하다고 하는 기술은 없다.\n",
    "상용 NLP 대부분이 기계번역에서 신경망 방식으로 대체됨\n",
    "음성인식, 이미지 분야에 비해 가장 늦게 시작했지만 가장 빨리 상용화\n",
    "\n",
    "<어텐션 시대>\n",
    "- 구글에서 나온 Transformer by End-to-End Attention\n",
    "기존의 LSTM, RNN과 달리 어텐션만 통해서 네트워크 구성\n",
    "성능이 월등히 뛰어나고, 확장성도 용이\n",
    "더 많은 데이터를 넣어서 더 큰 네트워크를 만들 수 있는 토대가 된다.\n",
    "지금은 어텐션으로 시작해서 어텐션으로 끝난다고 할 수 있음\n",
    "\n",
    "<BERTology: Pretraining and Fine-tuning>\n",
    "Big Language Models mainly based on Transformer\n",
    "트랜스포머에 기반한 빅모델\n",
    "-> 시퀀스투시퀀스, 어텐션, 트랜스포머는 새로운 아키텍쳐나 테크닉을 통해서 네트워크가 가진 성능의 한계를 끌어올린 것이라고 할 수 있음.\n",
    "이에 반해 버트는 네트워크를 개선했다기보다 그전에 있던 네트워크들을 크고 깊게 쌓고 그것을 어떻게 잘 옵티마이즈할 수 있는지를 주로 연구\n",
    "새로운 혁신을 제시했다고 보기에는 아쉬움이 있다.\n",
    "\n",
    "<what is next?>\n",
    "Open Domain Dialogue System(특정한 주제에 국한 X, 다양한 주제에서의 대화 시스템)\n",
    "Combining with Knowledge Graphs(딥러닝 이전에 많이 활용한 놀리지 그래프, 딥러닝 활용 방법도 주목받고 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Ch 02. Introduction - 07. 최근 흐름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 유명학회 : ACL, EMNLP\n",
    "Big Models / (Open Domain)Dialogue System / Question Answering / Combining with Knowledge Graphs / Natural Language Generations\n",
    "-> GPT3의 경우에는 다운스트림 태스크에 대해 파인튜닝 없이도 그냥 가져다 쓰기만 해도 좋은 성능을 내고 있다고 하는 수준.\n",
    "-> Konwledge Distillations : 버트는 돌리는 것도 버겁다고 함. 훨씬 더 경량화된 네트워크 모델 필요. 그래서 많이 연구되고 있는 것이 Konwledge Distillations(증류). 꼭 필요한 지식들만 증류해 모델 경량화.\n",
    "-> Digging Attention : 어텐션 쌓는 의미는 무엇인가… 등 모델을 설명하기 위한 연구. 어텐션 자체를 깊게 파는.\n",
    "-> Dialogue System : 상용화된 챗봇은 뉴럴 네트워크를 쓴 기술이 아니라 규칙기반이 사람을 갈아넣어서 만든 그런 것들 많다.\n",
    "-> 그래프 뉴럴 네트워크 연구되기도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Ch 03. Preprocessing - 01. 전처리 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<전처리의 늪에 오신 것을 환영합니다>\n",
    "- 가장 재미없고 반복적인 끝이 없는 작업\n",
    "- 하지만 가장 중요 - 어쩌면 모델링 만큼\n",
    "-> State Of The Arts(SOTA)라고 하는 모델을 누구나 쓸 수 있지만 데이터는 공개하지 않는다.\n",
    "-> 데이터의 양과 품질에서 성능 차이 난다.\n",
    "-> 전처리, 언어적 특성에 따르게 된다.\n",
    "\n",
    "<NLP Project Workflow>\n",
    "- 문제 정의\n",
    "단계를 나누고 simplify\n",
    "x와 y를 정의\n",
    "- 데이터 수집\n",
    "문제 정의에 따른 수집\n",
    "필요에 따라 레이블링\n",
    "- 데이터 전처리 및 분석\n",
    "형태를 가공\n",
    "필요에 따라 EDA 수행\n",
    "- 알고리즘 적용\n",
    "가설을 세우고 구현/적용\n",
    "- 평가\n",
    "실험 설계\n",
    "테스트셋 구성\n",
    "- 배포\n",
    "RESTful API를 통한 배포\n",
    "상황에 따라 유지/보수\n",
    "\n",
    "<Preprocessing Workflow / 데이터 수집, 데이터 전처리 및 분석>\n",
    "- 데이터(코퍼스) 수집\n",
    "구입, 외주\n",
    "크롤링을 통한 수집\n",
    "- 정제\n",
    "태스크에 따른 노이즈 제거\n",
    "인코딩 변환(혼재된 인코딩을)\n",
    "- 레이블링\n",
    "문장마다 또는 단어마다 레이블링 수행\n",
    "- Tokenization\n",
    "형태소 분석기를 활용해 분절 수행\n",
    "- Subword Segmentation(Optional) / 딥러닝에 와서 생긴 기법, 생략하기도 함\n",
    "단어보다 더 작은 의미 단위 추가 분절 수행\n",
    "- Batchify\n",
    "사전 생성 및 word2index 맵핑 수행\n",
    "효율화를 위한 전/후처리\n",
    "\n",
    "<말뭉치(corpus)란?>\n",
    "-자연어 처리를 위한 문장들로 구성된 데이터셋\n",
    "-복수 표현: Corpora\n",
    "-포함된 언어 숫자에 따라\n",
    "Monolingual Corpus\n",
    "Bi-lingual Corpus\n",
    "Multilingual Corpus\n",
    "-Parallel Corpus : 대응되는 문장쌍이 labeling돼 있는 형태\n",
    "English / Korean\n",
    "I love to go to school / 나는 학교에 가는 것을 좋아한다\n",
    "I am a doctor / 나는 의사입니다.\n",
    "-> 이러한 코퍼스가 있어야 기계번역할 수 있다. QA도 마찬가지.\n",
    "\n",
    "<Service Pipeline>\n",
    "-정제\n",
    "학습 데이터와 같은 방식의 정제 수행\n",
    "-토크나이제이션\n",
    "학습 데이터와 같은 방식의 분절\n",
    "-subword segmentation\n",
    "학습 데이터로부터 얻은 모델을 활용해 똑같은 분절 수행\n",
    "-Batchify\n",
    "학습데이터로부터 얻은 사전에 따른 word2index 맵핑 수행\n",
    "-Prediction\n",
    "모델에 넣고 추론 수행\n",
    "필요에 따라 search 수행(자연어 생성)\n",
    "-Detokenization(Optional)\n",
    "사람이 읽을 수 있는 형태로 변환(index2word)\n",
    "분절 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Ch 03. Preprocessing - 02. 코퍼스 수집, Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<데이터 구입 및 외주의 한계>\n",
    "-구입\n",
    "정제 및 레이블링이 완료된 양질의 데이터를 얻을 수 있음\n",
    "양이 매우 제한적\n",
    "구입처: 대학교, 한국전자통신연구원(ETRI), 플리토 등\n",
    "-외주\n",
    "수집, 정제 및 레이블링을 외주 줄 수 있음\n",
    "가장 높은 비용 -> 양이 매후 제한적\n",
    "품질 관리를 위한 인력이 추가로 필요\n",
    "\n",
    "-> 최대 10만~100만 단위\n",
    "\n",
    "<무료 공개 데이터>\n",
    "-공개 사이트\n",
    "AI-HUB**\n",
    "WMT competition\n",
    "Kaggle\n",
    "OPUS(http://opus.nlpl.eu/)\n",
    "-마찬가지로 양이 매후 제한적\n",
    "-한국어 코퍼스는 흔치 않음\n",
    "\n",
    "-> 최대 10만~100만 단위\n",
    "\n",
    "<Crawling>\n",
    "-무한한 양의 코퍼스 수집 가능\n",
    "원하는 도메인 별로 수집 가능\n",
    "-하지만 품질이 천차만별이며, 정제 과정에 많은 노력 필요\n",
    "e.g. 특수문자, 이모티콘, 노이즈, 띄어쓰기\n",
    "\n",
    "괜찮은 상용번역기를 만드려면 1000만 문장 이상 돼야 함.\n",
    "챗봇도 몇십만 문장으로는 택도 없다. 기계 번역기보다 더 필요 기본 1000만 이상\n",
    "\n",
    "-아직은 회색지대 하지만 적법한 절차에 따른 크롤링이 필수\n",
    "-robots.txt\n",
    "\n",
    "저작권이 존재하는 코퍼스로부터 학습한 모델과 그 생성물의 저작권은 누가 갖는가?\n",
    "\n",
    "<수집처>\n",
    "상용으로 할 때 블로그 기사 위키피디아 나무위키 등 크롤링해서 쓰면 안 됨. 다 저작권 있으니.\n",
    "수집처(블로그,네이버지식인,뉴스기사,위키피디아.나무위키.커뮤니티,테드.자막)/도메인(일반,다양,시사)/문체(대화체,문어체)/수집 난이도(덤프 제공,낮음,중간,높음)/양방향(번역)/정제난이도(낮음,중간,높음,최상)\n",
    "\n",
    "욕, 인종차별, 높임법, 역사(동해 일본해 표기) 등 유념해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Ch 03. Preprocessing - 03. 코퍼스 정제, Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<투 스텝>\n",
    "-기계적인 노이즈 제거\n",
    "전각문자 변환\n",
    "태스크에 따른 (전형적인) 노이즈 제거\n",
    "-Interactive 노이즈 제거\n",
    "코퍼스의 특성에 따른 노이즈 제거\n",
    "작업자가 상황을 확인하며 작업 수행\n",
    "\n",
    "<주의할 점>\n",
    "-태스크에 따른 특성\n",
    "풀고자 하는 문제의 특성에 따라 전처리 전략이 다름\n",
    "신중한 접근이 필요 e.g. 이모티콘은 필요 없는 정보일까?\n",
    "-언어, 도메인, 코퍼스에 따른 특성\n",
    "각 언어, 도메인, 코퍼스 별 특성이 다르므로 다른 형태의 전처리 전략이 필요\n",
    " \n",
    "<전각문자 제거>\n",
    "-유니코드 이전의 한글, 한자, 일본어는 전각 문자로 취급됐음\n",
    "-한자 등과 함께 표기된 반각 문자로 표기 가능한 전각 문자의 경우, 반각 문자로 치환\n",
    "중국어, 일본어 문서의 경우 많은 경우 전각 치환 필요\n",
    "오래된 한국어 문서의 경우 종종 전각 치환 필요\n",
    "-전각 문자의 예:\n",
    "!”#$%&\n",
    "01234\n",
    ":;<=>?@\n",
    "ABCDEF\n",
    "[\\]^_`\n",
    "abide\n",
    "{|} ~\n",
    "\n",
    "<대소문자 통일(Optional)>\n",
    "-코퍼스에 따라 대소문자 표기법이 다름\n",
    "-하나의 단어를 다양하게 표현하면 희소성이 높아짐\n",
    "-딥러닝의 시대에 오면서 필요성 하락 및 생략 가능\n",
    "New York City : NYC, nyc, N.Y.C, Nyc\n",
    "\n",
    "<정규식을 활용한 정제>\n",
    "- 정규식(regular expression)을 활용하면 복잡한 규칙의 노이즈도 제거/치환 가능\n",
    "- 코딩 없이 단순한 텍스트 에디터(sublime Text, VSCode 등)으로도 수행 가능\n",
    "전화번호 카드번호 등 다 제거해줘야 한다.\n",
    "\n",
    "<Interactive 노이즈 제거 과정>\n",
    "-규칙에 의해 노이즈를 제거하기 때문에 노이즈 전부를 제거하는 것은 어려움\n",
    "노이즈 확인 -> RegEx 구현 -> RegEx 적용\n",
    "-따라서 반복적인 규칙 생성 및 적용 과정이 필요\n",
    "-끝이 없는 과정\n",
    "노력과 품질 사이의 trade-off\n",
    "Sweet spot을 찾아야 함\n",
    "작은 정도의 노이즈는 딥러닝을 믿고 가줘야 하고, 어느 정도의 타협 필요\n",
    "\n",
    "<요약>\n",
    "-전처리 과정은 태스크와 언어, 도메인과 코퍼스의 특성에 따라 다르다\n",
    "-시간과 품질 사이의 트레이드오프\n",
    "-따라서 전처리 중에서도 특히 데이터 노이즈 제거의 경우, 많은 노하우가 필요\n",
    "요즘 같이 모델의 방법을 공유하는 시대에는 데이터 품질 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Ch 03. Preprocessing - 04. 정규식 (Regular Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<실무 팁: RegEx 적용 방법>\n",
    "Text Editor 활용\n",
    "파일을 열어 적용 과정을 보면서 정제\n",
    "바로 결과를 확인할 수 있음\n",
    "적용 과정이 log로 남지 않음 - 재활용 불가\n",
    "전용 모듈 작성 및 활용\n",
    "파이썬 등을 활용해 모듈을 만들고\n",
    "regex 리시트를 파일로 받아서 처리\n",
    "한번에 모든 regex를 적용 - 중간 결과 확인 불가\n",
    "regex 재활용 가능\n",
    "\n",
    "<텍스트 에디터 위드 RegEx>\n",
    "-Sublime Text 3, VSCode\n",
    "무료\n",
    "-EmEditor\n",
    "유료\n",
    "다양한 인코딩 지원\n",
    "대용량 코퍼스(기가바이트 단위) 로딩 가능\n",
    "\n",
    "[]\n",
    "2,3,4,5,c,d,e 중의 캐릭터\n",
    "[2345cde]\n",
    "(2|3|4|5|c|d|e)\n",
    "\n",
    "[-]\n",
    "2,3,4,5와 c,d,e 중의 캐릭터\n",
    "[2-5c-e]\n",
    "\n",
    "^\n",
    "2,3,4,5와 c,d,e,를 제외한 모든 캐릭터\n",
    "[^2-5c-e]\n",
    "\n",
    "()\n",
    "x를 \\1에 지정, yz를 \\2에 지정\n",
    "(x)(yz)\n",
    "\n",
    "<RegEx의 꿀기능>\n",
    "-양 끝에 알파벳(소문자)으로 둘러싸인 ‘bc’를 제거하기\n",
    "abcd\n",
    "0bc1\n",
    "\n",
    "-적용 예제\n",
    "([a-z]bc[a-z]) -> \\1\\2\n",
    "abcd -> ad\n",
    "0bc1 -> 0bc1\n",
    "\n",
    "\\0도 있음. 전체의 스트링을 0번 변수로 불러와서 쓸 수 있음.\n",
    "\n",
    "| (or)\n",
    "x또는 y가 나타남. 그리고 \\1에 지정\n",
    "(x|y)\n",
    "\n",
    "? (반복 횟수)\n",
    "x가 0번 또는 1번 나타남\n",
    "x?\n",
    "\n",
    "+ (반복 횟수)\n",
    "x가 한 번 이상 나타남\n",
    "x+\n",
    "\n",
    "*\n",
    "x가 나타나지 않을 수도, 반복될 수도 있음\n",
    "강력한 표현, 유의해서 사용해야 함\n",
    "x*\n",
    "\n",
    "{n}, {n.}, {n,m}\n",
    "n번 반복\n",
    "x{n}\n",
    "예 전화 번호 : [0-9]{3}\\-[0-9]{4}\n",
    "\n",
    "n번 이상 반복\n",
    "x{n,}\n",
    "x{10,}\n",
    "\n",
    "n번부터 m번까지 반복\n",
    "x{n,m}\n",
    "x{10,15}\n",
    "\n",
    ".\n",
    "any character\n",
    "매우 강력한 표현. 유의해서 사용해야 함\n",
    ".{9} 아무 캐릭터나 9개\n",
    ".+ 어떤 캐릭터가 한 번 또는 여러번 나타남\n",
    "\n",
    "^$(start of line - end of line)\n",
    "문장의 시작과 끝을 표시\n",
    "^x$\n",
    "\n",
    "<그 밖의 지정 문자>\n",
    "\\s 공백 문자 white space\n",
    "\\S 공백 문자를 제외한 모든 문자\n",
    "\\w alphanumeric(알파벳 + 숫자) + ‘_’ ([A-Za-z0-9_]와 같음)\n",
    "\\W non-alphanumeric 문자 및 ‘_’ 제외 (([^A-Za-z0-9_]와 같음)\n",
    "\\d 숫자 ([0-9]와 같음)\n",
    "\\D 숫자를 제외한 모든 문자 ([^0-9]와 같음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Ch 03. Preprocessing - 05. 실습 정규식 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python refine.py refine.regex.txt 1 < review.sorted.uniq.tsv > review.sorted.uniq.refined.tsv\n",
    "^(positive|negative)\\t[^\\t\\n]+\\t로 예상하지 못했던 오류 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Ch 03. Preprocessing - 06. 코퍼스 레이블링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Label>\n",
    "-Text Classification\n",
    "Input : sentence\n",
    "output: class\n",
    "-Token Classification\n",
    "Input: sentence\n",
    "output: tag for each token -> sequence\n",
    "-Sequence to Sequence\n",
    "input: sentence\n",
    "output: sentence\n",
    "\n",
    "<Label Example>\n",
    "-sentence -> class\n",
    "TSV(Tab Separate Value) 형태의 하나의 파일\n",
    "각 row가 문장과 대응되는 레이블\n",
    "문장 컬럼과 레이블 컬럼 구성\n",
    "예) 클래스 / 탭 / 센텐스\n",
    "\n",
    "-센텐스 -> 센텐스(시퀀스)\n",
    "TSV 형태의 하나의 파일\n",
    "각  row가 대응되는 문장 쌍\n",
    "각 문장별로 컬럼을 구성\n",
    "예) 센텐스 / 탭 / 센텐스\n",
    "\n",
    "\n",
    "두 개 이상의 파일로 구성\n",
    "같은 순서의 row가 대응되는 문장 쌍\n",
    "한 문장당 여러 레이블이 존재할 경우\n",
    "e.g. 한국어 <->영어<->중국어\n",
    "\n",
    "*NLP에서는 CSV(comma Separate Value) 안 쓰고 TSV 쓴다\n",
    "원래 문장 내에 콤마가 들어 있을 수 있기 때문에\n",
    "\n",
    "<팁: 레이블링 직접 진행하기>\n",
    "-휴먼 레이블링은 prototyping 시, 굉장히 강력한 도구(두려워하지 말자)\n",
    "-효율적인 레이블링 도구를 구성하자(예. 엑셀)\n",
    "비지도학습 방법으로 상용 프로그램을 만드는 것은 거의 불가능하다.\n",
    "레이블링 직접 진행하는 게 훨씬 더 나은 방법.\n",
    "휴먼 레이블링 성능을 따라 올 수 없다.\n",
    "생각보다 할 만하다. 혼자하면 힘들긴 하지만\n",
    "적은 수로 한번 해보고 그 다음 성능을 쥐어짤 때 비지도학습, 규칙기반 방법 고려\n",
    "데이터를 룰을 가지고 오그멘테이션했다고 하면 결국 딥러닝 모델은 이 규칙을 학습하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Ch 03. Preprocessing - 07. 한,중,영,일 코퍼스 분절(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<투 스텝>\n",
    "센텐스 세그멘테이션\n",
    "토크나이제이션\n",
    "\n",
    "<센텐스 세그멘테이션>\n",
    "-보통 훈련 시 우리가 원하는 입력 데이터는\n",
    "1센텐스 /라인\n",
    "-우리가 수집한 코퍼스는\n",
    "한 라인에 여러 문장이 들어있거나\n",
    "한문장이 여러 라인에 들어 있음\n",
    "-센텐스 세그멘테이션을 통해 원하는 형태로 변환\n",
    "마침표 등을 단순히 문장의 끝으로 처리하면 안 됨!\n",
    "예) 3.141592, U.S.\n",
    "-NLTK(네추럴 랭귀지 툴킷)를 활용해 변환 가능\n",
    "from nltk.tokenize import sent_tokenize / 영어를 기준으로 만들어진 센텐스 토크나이저\n",
    "\n",
    "Multiple sentence/line\n",
    "Multiple line/sentence 자막의 경우\n",
    "\n",
    "<Tokenization>\n",
    "-왜?\n",
    "두 개 이상의 다른 토큰들의 결합으로 이루어진 단어를 쪼개어, 단어 순자를 줄이고, 희소성(sparseness)을 낮추기 위함.\n",
    "\n",
    "-예\n",
    "Korea /’s\n",
    "mouthpiece /,\n",
    "\n",
    "<Korean Tokenization>\n",
    "왜?\n",
    "교착어: 어근에 접사가 붙어 다양한 단어가 파생됨\n",
    "띄어쓰기 통일의 필요성\n",
    "\n",
    "<토크나이제이션 for other languages>\n",
    "영어: 띄어쓰기가 이미 잘 돼 있음. NLTK를 사용해 콤마 등 후처리\n",
    "중국어: 기본적인 띄어쓰기가 없음. 캐릭터 단위로 사용해도 무방\n",
    "일본어: 기본적인 띄어쓰기가 없음.\n",
    "\n",
    "<형태소 분석 및 품사 태깅(Part of Speech Tagging, POS 태깅)\n",
    "-형태소 분석: 형태소를 비롯해 어근, 접두사/접미사, 품사(POS) 등 다양한 언어적 속성의 구조를 파악하는 것\n",
    "-품사 태깅: 형태소의 뜻과 문맥을 고려해 그것에 마크업을 하는 일\n",
    "\n",
    "<POS Tagger for Other Languages>\n",
    "한국어 Mecab  일본어 Mecab을 래핑. 속도가 가장 빠름\n",
    "한국어 KoNLPy 설치와 사용이 편리하나, 일부 태거의 경우 속도가 느림\n",
    "일본어 Mecab 속도가 가장 빠름\n",
    "중국어 Stanford Parser 미국 스탠포드에서 개발\n",
    "중국어 PKU Parser 북경대학교에서 개발\n",
    "중국어 Jieba 가장 최근에 개발 파이썬으로 제작돼 시스템 구성에 용이\n",
    "\n",
    "<품사 태깅 예제 feat. Mecab>\n",
    "아버지가 방에 들어가신다.\n",
    "아버지 가방에 들어가신다.\n",
    "\n",
    "<분절 예제(feat. Mecab with -O wakati)>\n",
    "-메캡에서는 와카티 옵션 주면 애초에 세그멘테이션만 제공한다\n",
    "$ echo ‘아버지가 방에 들어가신다.’ | mecab -O makati\n",
    "아버지 가 방 에 들어가 신다 .\n",
    "\n",
    "<요약>\n",
    "-한국어의 경우\n",
    "접사를 분리해 희소성을 낮추고\n",
    "띄어쓰기를 통일하기 위해 토크나이제션을 수행\n",
    "-굉장히 많은 POS Tagger가 존재하는데\n",
    "전형적인 쉬운 문장(표준 문법을 따르며, 구조가 명확한 문장)의 경우, 성능이 비슷함\n",
    "하지만 신조어나 고유명사를 처리하는 능력이 다름\n",
    "따라서, 주어진 문제에 맞는 정책을 가진 태거를 선택해 사용해야 함\n",
    "내가 하는 태스크가 어떤 POS 태거의 정책에 더 맞는지를 판단해서 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
