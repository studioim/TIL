{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40. Ch 04. Word Embedding - 14. Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Myth>\n",
    "-미리 스킵그램 등을 통해 계산한 워드 임베딩 벡터를 네트워크에 넣어주는 것이 아니다\n",
    "-보통은 원핫 벡터 자체를 넣어줘야 한다.\n",
    "\t-즉 단어 자체를 넣어주는 것\n",
    "\t-N개의 단어가 있다면, N+알파 차원의 벡터가 네트워크의 입력이 된다.\n",
    "임베딩 레이어를 거치면 자연스럽게 워드 임베딩 벡터가 나오게 됨.\n",
    "\n",
    "<임베딩 레이어>\n",
    "-원핫벡터s(k,n)  X 임베딩 레이어 weight(n,m) = 임베딩 벡터s(k,m)\n",
    "-(Batch size, Vocab size) X (Vocab size, 임베딩 벡터 사이즈) = (배치 사이즈, 임베딩 벡터 사이즈)\n",
    "\n",
    "그냥 리니어 레이어에 원핫 벡터를 넣어놓은 것.\n",
    "조금 계산의 효율을 높여놓은 것.\n",
    "강아지, 개, 멍멍이 등은 학습할 수록 비슷한 워드 임베딩 벡터가 나올 것임.\n",
    "이런식으로 임베딩 레이어를 최적화할 수 있는 것.\n",
    "\n",
    "워드투벡을 통해서 미리 학습된 임베딩 벡터를 넣을 수 있다.\n",
    "그런데 워드투벡은 주변 단어를 잘 예측하는 것이 목표. 이것에 최적화된 워드 임베딩 벡터.\n",
    "내가 할 태스크는 다양(번역, 감성분석, 챗봇 등)하다.\n",
    "다양한 태스크에 따른 각 단어의 역할은 다르다. 그때의 목표도 다르다.\n",
    "역전파돼서 학습이 되는 임베딩 레이어 웨이트가 그때마다 다르게 학습된다.\n",
    "프리트레인 워드임베딩 벡터를 사용하면 최선이 아닐 수 있다.\n",
    "그래서 보통은 원핫벡터를 넣어준다.\n",
    "\n",
    "<Why Embedding Layer>\n",
    "-원핫벡터 x -> 임베딩 레이어 -> 임베딩 벡터 Z -> 레이어 -> 활성함수 -> 레이어 -> 활성함수 -> y hat\n",
    "리니어는 아무리 깊게 쌓아도 리니어\n",
    "그러므로 뉴럴 네트워크 이전 임베딩 과정은 그냥 하나의 리니어와 같다고 볼 수 있다.\n",
    "그렇다면 임베딩 레이어를 왜 쓰냐? Computation Efficiency\n",
    "\n",
    "사실은 임베딩 레이어 구현할 때 인풋에 원핫벡터가 아니라 원핫이 있는 인덱스를 넣어준다\n",
    "실제 원핫벡터와 임베딩레이어웨이트 간의 내적도 해줄 필요가 없다. 엄청나게 큰 매트릭스(행렬) 곱이 발생하기 때문.\n",
    "그래서 인덱스에 해당하는 row만 가져오도록 구현해 놓은 것. 수학적으로 똑같다.\n",
    "임베딩 레이어를 두는 것은 수학적으로 큰 차이가 없지만 레이어 하나로 표현하는 것보다 계산 효율성으로 봤을 때 효율이 높다.\n",
    "\n",
    "<요약>\n",
    "-임베딩 레이어를 통해 원핫벡터를 각 태스크에 맞는 dense representation으로 바꿀 수 있음\n",
    "\t-단순히 fix된 워드 임베딩 벡터 자체(예. 워드투벡)를 넣어줄 경우, 해당 알고리즘의 objective에 최적화한 representation을 넣어주는 것임.\n",
    "-특수한 경우(코퍼스가 부족하다)에 한해\n",
    "\t-fixed word representation을 사용하거나\n",
    "\t-워드 임베딩 벡터들로 구성된 웨이트 파라미터를 seed로 사용해 최적화\n",
    "-하지만 베이스라인은 원핫벡터를 넣어주는 것\n",
    "-그럼 워드 임베딩 알고리즘은 언제 쓰나요? 사실 거의 안 쓴다\n",
    "딥러닝은 엔드투엔드 시스템을 구성하고 싶어한다. 그런데 임베딩 벡터는 보통 최종 산출물이 되지 않고 중간 산출물이 되기 때문.\n",
    "개념은 중요하지만 실무에서는 잘 안 쓰임. 한 단어가 어떤 단어와 유사한지 보는 정도로 쓰임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T15:34:32.553706Z",
     "start_time": "2021-01-18T15:34:32.549082Z"
    }
   },
   "source": [
    "# TODO : 41. Ch 04. Word Embedding - 15. 타 분야 적용 사례"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
