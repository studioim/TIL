{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리(NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "챗봇, 클린봇, 개인 비서 서비스\n",
    "\n",
    "의외로 자연어 처리가 아닌 것들\n",
    "자동 음성 인식(ASR, Automated Speech Recognition)\n",
    "광학 문자 인식(OCR, Optical Character Recognition)\n",
    "자연어의 의미를 이해하지 않으므로 자연어처리와는 다르다\n",
    "단 최근에는 NLP 기술과 결합해 ASR과 OCR의 성능을 향상시키고 있다\n",
    "\n",
    "자연어처리 : 복잡성(complexity), 애매함(Ambiguity), 의존성(dependency)\n",
    "사람도 사람의 말을 이해하기 어렵다. 사람은 사전 지식(a priori), 다중 모드(multi mode), 상호 작용(interaction) 등으로 이를 해결한다.\n",
    "\n",
    "머신러닝\n",
    "도메인 지식을 상당히 요하기 때문에(언어학), 진입 장벽이 높고 기대효과가 분명. 보통 SVM 많이 사용했다. 분류등에서.\n",
    "\n",
    "딥러닝\n",
    "인공지능 기술이 핵심이 되며, 도메인 지식의 상당 부분을 데이터 드리븐으로 해결해 의외의 인사이트를 얻을 수 있다.(딥러닝 위주)\n",
    "\n",
    "-문장번역 (머신 트랜스레이션)\n",
    "-감정 분석(sentiment analysis) : 금융 또는 경영에서 판단에 도움을 받기 위해 사용(positive, negative, neutral)\n",
    "-챗봇 : 머신러닝, 딥러닝 다양하게 이용. 챗봇은 문맥을 이해하고 다양한 답변을 생성해야 하므로 다양한 기술을 종합해야 한다.\n",
    "-문맥 광고(contextual advertising) : 광고주와 콘텐츠 게시자 사이의 매칭을 이루게 해주는.  광고 시장은 굉장히 큰 시장이다.\n",
    "유튜브도 광고 수익으로 모두 정산을 받고 있는 것. 광고는 영원히 죽지 않는 산업.\n",
    "구글 자율주행차 산업도 운전 하는 시간 줄어들면 그만큼 다른 콘텐츠에 시간을 소비하고 이를 통해 더 많은 광고를 소비할 수 있기 떄문.\n",
    "\n",
    "단어를 숫자로 표현하기\n",
    "컴퓨터는 아스키, 유니코드, UTF-8 인코딩 등으로 문자를 표현하고 저장한다.\n",
    "한글은 유니코드 UTF-8에 포함.\n",
    "\n",
    "컴퓨터가 보는 단어\n",
    "\n",
    "원핫 인코딩\n",
    "N개의 단어를 좌측의 코드(0, 1, 2, 3, 4, 5 …)로 표현하면 희소 표현(sparse representation)이라고 하며,\n",
    "우측의 벡터로 표현할 경우 원핫 인코딩이라고 한다.(0010 식으로, 희소 벡터)\n",
    "\n",
    "밀집 표현 Dense representation\n",
    "희소 표현(001…0) -워드임베딩-> 밀집 표현(0.4, 0.7, .., -0.1)\n",
    "희소 표현된 단어를 임의의 길이의 실수 벡터로 표현할 경우, 이를 밀집 표현이라고 한다. 이 과정을 워드 임베딩이라고 하며,\n",
    "밀집된 표현 결과를 임베딩 벡터라고 부른다.\n",
    "희소표현에서 밀집 표현으로 갈수록 일반적으로 낮은 차원으로 바뀌게 된다.\n",
    "낮은 차원으로 표현이 되게 되면 서로간의 관계를 알아낼 수 있게 된다?\n",
    "\n",
    "말뭉치(코퍼스)\n",
    "특정 목적을 가진 언어의 표본. 분석의 용이성을 위해 형태소 분석이 포함되기도 한다.\n",
    "언어학 연구에 쓰이는 확률/통계적 자료이며, 딥러닝에도 유용하게 쓰인다\n",
    "우리나라 말뭉치는 잘 구성돼 있지 않다. 미국은 잘 정리돼 있다.\n",
    "한국은 최근 들어 수집되고 있긴 하지만 오랫동안 쌓여온 것이 아니라 레이블링 일관되지 않은 경우도 있고,\n",
    "사람마다 다르게 돼 있고, 연구하기가 쉽지 않다고 함.\n",
    "\n",
    "워드투벡\n",
    "구글에서 나온 방법. 가장 많이 사용되는 워드 임베딩 방법이다\n",
    "워드투벡은 유사한 의미를 가진 단어는 유사한 벡터가 되는 특징이 있다.\n",
    "\n",
    "CBOW(continuous Bag of words)\n",
    "CBOW 모델은 주변의 단어로 현재 단어를 추정하는 방법이다(윈도우 사이즈 = 2)\n",
    "sparse-dense-sparse\n",
    "크게 프로젝션 레이어와 아웃풋 레이어로 구분된다\n",
    "로스 펑션은 정답인 원핫 벡터와 크로스 엔트로피 로스를 이용한다.\n",
    "\n",
    "Skip-Gram\n",
    "CBOW와 반대로, 윈도우 중앙에서 주변 단어를 추정하는 방식이다.\n",
    "일반적으로 CBOW보다 skip-gram 모델의 성능이 좋은 것으로 알려져 있다.\n",
    "\n",
    "형태소 분석기\n",
    "형태소(Morpheme)\n",
    "언어학적으로 말을 분석할 때, 의미가 있는 가장 작은 말의 단위\n",
    "의존성 : 자립형태소(한나 ,책), 의존형태소(가, 을, 보, 았, 다)\n",
    "의미 여부 : 실질 형태소(한나, 책, 보), 형식 형태소\n",
    "\n",
    "형태소 분석\n",
    "문장을 형태소 단위로 구분하고 언어적 구조를 파악하는 것을 형태소 분석이라고 한다.\n",
    "어근, 잡두사/접미사, 품사(POS: Part-of Speech) 등을 구분한다.\n",
    "\n",
    "형태소 분석기\n",
    "KoNLPy(한나눔, 꼬꼬마, 코모란, MeCab-Ko, OKT(Twitter))\n",
    "자바스크립트로 개발된 다양한 형태소 분석기의 파이선 Wrapper이다.\n",
    "(자바스크립트로 개발됐긴하지만 파이선으로 싸여져 있어서 파이선 문법만으로도 사용할 수 있다는 개념)\n",
    "다양한 한글 형태소 분석기를 통일된 방법으로 쉽게 사용할 수 있다.\n",
    "\n",
    "한나눔 카이스트에서 개발된 형태소 분석기(1999)\n",
    "analyze - morphs - nouns - pos(Tagger)\n",
    "\n",
    "꼬꼬마 서울대\n",
    "morphs-nouns-pos\n",
    "\n",
    "Okt twitter\n",
    "morphs- normalize - nouns - phrases - pos\n",
    "\n",
    "seq2seq(attention)\n",
    "인코더와 디코더가 콘텍스트로 연결돼 있음\n",
    "임베딩 - 인코더(RNN) - 콘텍스트 - 디코더(RNN) - DENSE - 소프트맥스\n",
    "시퀀스투시퀀스 모델은 번역문제를 학습하기 위해 널리 사용되고 있는 RNN 구조이다.\n",
    "\n",
    "콘텍스트를 개선하려면?\n",
    "인코더 히든 스테이트를 모아서 디코더로 전달하면 콘텍스트를 향상시킬 수 있을 것!\n",
    "그래서 필요한 것이 어텐션 메커니즘\n",
    "\n",
    "쿼리, 키-밸류\n",
    "쿼리:질의 찾고자 하는 대상\n",
    "키 : 저장된 데이터를 찾고자 할 때 참조하는 값\n",
    "밸류: 값, 저장되는 데이터\n",
    "딕셔너리 : 키-밸류 페어로 이루어진 집합\n",
    "\n",
    "querying\n",
    "\n",
    "어텐션 메커니즘\n",
    "Q에 대해 어떤 K가 유사한지 비교하고, 유사도를 반영해 V들을 합성한 것이 Attention value이다.\n",
    "쿼리 -> Comparison -> Aggregation -> Attention Value\n",
    "\n",
    "시퀀스투시퀀스에서는 디코더의 히든 레이어들을 쿼리로 사용. 주의할 점은 인코더와 달리 하나 앞선 타임-스텝의 히든 레이어를 사용한다는 점.\n",
    "\n",
    "ConvNet을 이용한 문장 분류\n",
    "철 지난 방법이긴 하지만, 쉽게 구현해서 확인해보기엔 좋은 방법. 경우에 따라서는 단순한 네트워크 이용해서 써봐도 좋다.\n",
    "\n",
    "합성곱 신경망\n",
    "이미지 -> (합성곱계층-풀링계층-활성함수) * n번 반복 -> (평탄화-전결합계층-전결합계층) -> 분류 결과\n",
    "합성곱 신경망은 이미지 분류에 널리 사용되는 네트워크 구조이다.\n",
    "\n",
    "트랜스포머\n",
    "시퀀스투시퀀스와 유사한 트랜스포머 구조 사용\n",
    "제안하는 스케일드 닷 프로덕트 어텐션과, 이를 병렬로 나열한 멀티 헤드 어텐션 블록이 알고리즘의 핵심\n",
    "RNN의 BPTT와 같은 과정이 없으므로 병렬 계산 가능\n",
    "입력된 단어의 위치를 표현하기 위해 포지셔널 인코딩 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
