{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [04. Chapter4. Sequence-to-Sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Ch 04. Sequence-to-Sequence - 01. Machine Translation 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "시퀀스투시퀀스가 가장 먼저 두각을 나타내기 시작한 기계 번역에 대해\n",
    "\n",
    "<기계 번역>\n",
    "문장 x가 주어졌을 때 y가 나올 확률을 최대로 하는, 문장 집합 Y에서 y(우리가 원하는 번역문)의 값\n",
    "머릿속에서도 우리가 하고 있다.\n",
    "\n",
    "<역사>\n",
    "1950년부터 기계 번역 하려는 시도가 있었다.\n",
    "소련의 문서를 영어로 번역하려는 시도\n",
    "물론 택도 없었음\n",
    "이런 염원이 이어져 옴.\n",
    "\n",
    "-Rule-based MT(RBMT)\n",
    "사람은 새로운 데이터가 주어졌을 때 기존의 규칙들을 가지고 일반화해서 훌륭한 추론을 해낼 수 있다.\n",
    "하지만 컴은 일반화 능력이 0점.\n",
    "다른 언어 쌍에는 적용되지 않는 단점.\n",
    "\n",
    "-Statistical MT(SMT)통계기반 번역\n",
    "구글이 도입. 구글 트랜스레이션 널리 알려지게 된 계기\n",
    "\t-Phrased based MT(PBMT)\n",
    "\t거기서 더 발전해서 구와 절 간의 통계를 활용\n",
    "굉장히 복잡\n",
    "코퍼스가 있으면 가능. 확장 굉장히 쉬움.\n",
    "구글 번역기가 SMT 알고리즘 하나 만든 다음\n",
    "여러 언어에 대해 번역을 제공할 수 있었음.\n",
    "구글번역기가 널리 알려지게 된 계기.\n",
    "\n",
    "2014년 이전까지는\n",
    "음성 인식이나, 이미지 인식 분야에서는 엄청난 퀀텀점프가 이뤄짐.\n",
    "하지만 NLP는 별일 없었음.\n",
    "텍스트 분류하고 워드 임베딩하고 이게 다.\n",
    "\n",
    "그러다 2014년에\n",
    "시퀀스투시퀀스 발명되면서 Neural Machine Translation(NMT)를 비롯한 자연어 생성계에 혁명이 일어남\n",
    "딥러닝 활용한 NLP 연구가 활발히 꽃피우기 시작함.\n",
    "시퀀스투시퀀스가 가능해지면서 수치를 받아서 원하는 문장을 만들어낼 수 있게 됨.\n",
    "\n",
    "가장 먼저 혁명이 시작됐는데 가장 먼저 끝나버림.\n",
    "엔드투엔드 머신 트랜스레이션이 대세가 되어서 현재 상용화된 기계번역 시스템이라고 한다면 모두 NMT를 쓰고 있다고 보면 됨.\n",
    "음성 인식 같은 경우에는 일부만 바뀐 거였고\n",
    "엔드투엔드 같은 경우 상용화 추세긴 하지만 100퍼센트 가깝게 모두 대체됐다고 하기는 어려움\n",
    "그런데 기계번역 같은 경우에는 거의 대부분이 NMT로 대체\n",
    "가장 늦게 시작했는데 가장 먼저 상용화가 끝나\n",
    "언어라는 것이 컴퓨터 입장에서는 다른 분야에 비해 단순한 함수였던 것.\n",
    "\n",
    "<어떻게 딥러닝 자연어처리가 기존의 자연어처리를 압도할 수 있었나?>\n",
    "-엔드투엔드 모델(딥러닝이 궁긍적으로 지향하는 바)\n",
    "SMT방식은 여러 서브 모듈들이 진행될수록 에러가 가중됨\n",
    "서브 모듈들이 모두 성능이 100퍼센트가 아니므로\n",
    "엔드투엔드 모델은 데이터의 손실 없이 처음부터 끝까지 진행된다. 손해가 적을 수밖에 없음.\n",
    "\n",
    "-Better generalization\n",
    "Discrete한 단어를 continuous한 값으로 변환해 계산\n",
    "\t-워드 임베딩\n",
    "\t-컨텍스트 임베딩\n",
    "\n",
    "-LSTM(RNN 한계 극복)과 Attention의 적용\n",
    "시퀀스의 길이에 구애받지 않고 번역(문장이 길어지면 어려워지기 마련인데)\n",
    "\n",
    "이러한 세 가지 이유로 딥러닝의 NLP가 기존의 전통적 NLP보다 훨씬 더 압도적인 성능을 자랑할 수 있게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Ch 04. Sequence-to-Sequence - 02. Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder, Decoder, Generator 등 3가지의 아키텍처로 구성\n",
    "\n",
    "-Given dataset,\n",
    "-Find parameter that maximize likelihood,\n",
    "-Minimize loss function by updating parameter with gradient descent.\n",
    "이런 식으로 시퀀스투시퀀스 모델을 학습시키고 우리 머릿속에 있는 그라운드 트루스 번역 확률분포함수를 모사할 수 있다.\n",
    "\n",
    "인코더와 디코더 하니까 오토인코더가 떠오른다\n",
    "<리뷰: 오토인코더s>\n",
    "-인코더와 디코더를 통해 압축과 해제를 실행\n",
    "인코더는 입력(x)의 정보를 최대한 보존하도록 손실 압축을 실행\n",
    "디코더는 중간 결과물(z)의 정보를 입력(x)과 같아지도록 압축 해제(복원)를 수행\n",
    "-복원을 성공적으로 하기 위해 오토인코더는 특징을 추출하는 방법을 자동으로 학습\n",
    "오토인코더와 상당히 비슷, 그게 시퀀스를 다루고 있을 뿐.\n",
    "\n",
    "<애플리케이션>\n",
    "-자연어생성 태스크\n",
    "NMT, 챗봇, 요약(추출, 새로운 문장 생성 요약), Other NLP Task, ASR, Lip Reading, Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Ch 04. Sequence-to-Sequence - 03. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "인코더는 문장을 벡터로 만들어주는 일을 한다.\n",
    "이는 텍스트 클래서피케이션에서도 하는 것.\n",
    "인코더가 숫자(벡터로) 변환해서 넘겨주면 디코더는 이를 받아서 문장을 만들어준다.\n",
    "높은 차원의 데이터(문장)를 낮은 차원의 latent space에 한 점으로 찍어주게 된다.\n",
    "이를 디코더가 문장으로 만들어주는 것.\n",
    "\n",
    "문장은 단어들의 시퀀스\n",
    "입력과 출력의 텐서 모양은 뻔하다.\n",
    " <요약>\n",
    "-인코더는 소스 문장을 압축한 컨텍스트 벡터를 디코더에 넘겨준다.\n",
    "-인코더는 트레인/테스트 시에 항상 문장 전체를 받음.\n",
    "인코더 자체만 놓고 보면 non-auto-regressive task.\n",
    "따라서 bi-directional RNN 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Ch 04. Sequence-to-Sequence - 04. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "인코더가 잘 압축해놓은 컨텍스트 벡터를 받아서 문장을 만들어냄.\n",
    "인코더의 마지막 히든스테이트를 디코더의 이니셜 히든 스테이트로 넣어주게 됨.\n",
    "\n",
    "<요약>\n",
    "디코더는 원투매니 문제에 속함\n",
    "-디코더는 conditional language model이라고 볼 수 있음\n",
    "인코더로부터 문장을 압축한 context vector를 바탕으로 문장을 생성\n",
    "-Auto-regressive task에 속하므로, uni-directional RNN을 사용\n",
    "\n",
    "인코더는 바이디렉셔녈, 디코더는 유니디렉셔널을 사용하면서 생기는 문제가 발생(서로 히든스테이트 개수가 다르므로)\n",
    "이를 해결하는 방법도 파이토치를 활용해 실습할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Ch 04. Sequence-to-Sequence - 05. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "디코더로부터 히든 스테이트를 받아서 그 정보를 활용한 뒤 리니어 레이어와 소프트맥스를 거쳐서 그 타임 스텝의 단어를 예측(생성)하는 아키텍처\n",
    "Generator returns a probability distribution of current output token.\n",
    "미니배치 내 문장 별 현재 타임 스텝의 단어별 확률 값을 얻을 수 있다.\n",
    "\n",
    "이를 하기 위해서는 discrete value를 예측하는 분류이기 떄문에\n",
    "크로스 엔트로피를 쓰면 됨.\n",
    "\n",
    "인코더는 생성 태스크가 아님. 압축하는 태스크.\n",
    "생성하는(auto regressive) 태스크에서는 BOS, EOS가 필요\n",
    "\n",
    "밑에는 BOS가 있고 EOS가 없고, 위에는 그 반대.\n",
    "\n",
    "<요약>\n",
    "-제너레이터는 디코더의 히든 스테이트를 받아 현재 타임 스텝의 출력 토큰에 대한 확률 분포(multinoulli distribution) 반환\n",
    "-단어를 선택하는 문제이므로 크로스 엔트로피 로스를 통해 최적화 가능\n",
    "GT 분포와 모델 분포 사이의 차이를 최소화하기 위함\n",
    "조건부 언어모델로 볼 수 있으므로, PPL(Perplexity)로 치환 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: 06. Ch 04. Sequence-to-Sequence - 06. Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
