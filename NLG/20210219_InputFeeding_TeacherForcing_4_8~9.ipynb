{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Ch 04. Sequence-to-Sequence - 08. Input Feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "인풋 피딩은 디코더에서 이전 타임스텝의 소프트맥스에 들어가기 전 h틸다를 워드 임베딩 벡터에 concat해주는 것\n",
    "\n",
    "그렇게 해서 얻는 이점은\n",
    "1)만약 두 단어밖에 없다고 가정하면 h틸다의 소프트맥스 결과 값이 확률 6:4 정도로 확실하지 않았을 수 있다.\n",
    "그럼 디코더에서 이전 스텝의 실제 결과 값(원핫 벡터 1,0)을 다음 스텝의 인풋으로 받아들일 경우 다음 스텝 입장에서는 정보가 확실하지 않아진다.\n",
    "정보의 손실이 생기는 셈.\n",
    "이를 막기 위해 제일 좋은 방법은 h틸다나 소프트맥스 이후 결과 값이나 사실 같은 값이기 떄문에\n",
    "h틸다를 워드임베딩 벡터랑 concat해주겠다는 것.\n",
    "\n",
    "2)언어 모델이니까 오토리그레시브, 티처 포싱을 써야 된다.\n",
    "티처 포싱을 했을 때 문제점 중 하나가 학습과 트레이닝의 괴리가 생기는 것.\n",
    "소프트맥스 취한 이후 뱉어낸 yhat 값이 다음 타임 스텝의 인풋 값으로 들어가지 않기 때문에.\n",
    "학습할 때 이전 타임 스텝에서 뱉어낸 것은 상관없나보다 이런 식으로 잘못 학습할 수 있음.\n",
    "\n",
    "그런데 yhat을 concat해주면서(인풋 피딩) 이번 타임스텝의 실제 정답도 받아들이는…\n",
    "오토리그레시브 태스크로 인해서 생기는 티처 포싱의 문제점을 보완하는 방법이 됨.\n",
    "\n",
    "<By Input Feeding>\n",
    "-샘플링 과정에서 손실되는 정보를 최소화\n",
    "-티처 포싱으로 인한 학습/추론 사이의 괴리를 최소화\n",
    "\n",
    "<Wrap up>\n",
    "-Encoder\n",
    "문장을 받아 컨텍스트 벡터로 압축\n",
    "Bi-directional RNN을 통해 구현(Non Autoregressive Task)\n",
    "\n",
    "-Decoder & Generator\n",
    "Conditional Language Model\n",
    "\t-인코더로부터 정보를 받아 문장을 생성\n",
    "크로스 엔트로피(PPL)를 통해 최적화\n",
    "\t-언어모델이므로\n",
    "\n",
    "-Attention\n",
    "미분가능한 키-밸류 함수\n",
    "디코더의 히든 스테이트를 인코더의 각 히든 스테이트에 유사도 비교.\n",
    "\t-그 결과 내가 필요한 정보들을 인코더에서 빼올 수가 있음\n",
    "\t-weighted sum으로\n",
    "좋은 쿼리를 만들어내는 과정을 학습\n",
    "\n",
    "-Input Feeding\n",
    "샘플링 과정에서 손실된 정보를 워드 임베딩에 concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Ch 04. Sequence-to-Sequence - 09. Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "시퀀스투시퀀스는 언어모델에 해당\n",
    "언어모델은 Auto regressive한 속성을 가지고 있다.\n",
    "이런 경우 추론은 이전 결과들을 입력으로 받아 추론한다.\n",
    "\n",
    "오토리그레시브: 과거 자신의 상태를 참조해 현재 자신의 상태를 업데이트\n",
    "그전에 잘못 추론하면 점점 더 틀리게 된다는 단점이 있음.\n",
    "\n",
    "티처포싱은 추론할 때 예측한 값이 아닌 원래 정답을 넣어준다.\n",
    "원래 추론하는 방법처럼 학습할 수 없다.\n",
    "\n",
    "<요약>\n",
    "-오토리그레시브 태스크를 피드 포워드할 때는 보통 이전 타임 스텝의 출력이 현재 타임스텝의 입력이 됨\n",
    "\n",
    "-티처 포싱을 통해 오토 리그레시브 태스크에 대한 시퀀셜 모델링을 할 수 있음\n",
    "하지만 트레이닝 모드와 인퍼런스 모드의 괴리(discrepancy)가 생김\n",
    "성능상의 로스가 생길 수 있다.\n",
    "\n",
    "-RL(강화학습)을 통해 이러한 괴리를 없애고 성능을 높일 수 있음\n",
    "이외에도 다양한 방법(예. professor forcing)들이 제안됨\n",
    "\n",
    "MLE를 통해 학습을 하고자 할 때는 티처포싱을 써야 한다!\n",
    "이거 하나만이라도 기억해라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: 10. Ch 04. Sequence-to-Sequence - 10. 실습 실습 소개"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
