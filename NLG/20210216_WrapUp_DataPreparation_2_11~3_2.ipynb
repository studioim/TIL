{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ch 02. Language Modeling - 11. 정리하며"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<언어 모델이란>\n",
    "-실제 우리가 사용하는(또는 타깃 도메인) 언어의 분포를 확률 모델로 모델링한 것\n",
    "체인 룰에 의해서 문장의 확률을 모델링하는 것은 단어들이 주어졌을 때, 다음 단어의 확률을 모델링하는 것과 같음\n",
    "-언어 모델을 통해 우리는 아래의 태스크를 수행할 수 있음\n",
    "1)주어진 문장들 중에서 가장 fluent한 문장을 골라낼 수 있다\n",
    "2)단어들이 주어졌을 때, 다음 단어를 확률적으로 예측할 수 있다.\n",
    "결국 같은 태스크를 하는 것\n",
    "\n",
    "<Perplexity>\n",
    "-매 타임스텝마다 모델이 동등하게 헷갈리고 있는 평균 단어의 수\n",
    "헷갈리는 단어가 적을수록 좋은 것 == lower is better\n",
    "-문장의 확률의 역수에 단어 수 만큼 기하평균을 취한 것\n",
    "문장의 likelihood가 높을수록 좋은 것 == lower is better\n",
    "-크로스 엔트로피에 exponential을 취한 것\n",
    "GT 분포와 모델의 분포가 비슷할수록 좋은 것 == lower is better\n",
    "\n",
    "<n-gram and Neural Network Language Model>\n",
    "뉴럴네트워크 이전의 방식으로 엔그램 배웠다.\n",
    "-n-gram\n",
    "\t-단어를 discrete symbol로 인식\n",
    "\tExact matching에 대해서만 count\n",
    "\t-학습 코퍼스에 워드 시퀀스가 존재해야만 확률 값을 추정 가능\n",
    "\t마코프 가정 도입\n",
    "\t-쉽고 직관적인 구현\n",
    "\t학습(counting) 후, 추론(table look-up)\n",
    "\tScalable하며, 저렴한 계산 비용\n",
    "-NNLM(RNN에 기반)\n",
    "\t-단어를 continuous vector로 변환\n",
    "\tUnseen word sequence에 대처 가능\n",
    "\tGeneralization에 강점\n",
    "\t-비싸고 느린 연산 추론 과정\n",
    "\t요샌 또 GPU가 있으니까\n",
    "\t-Generation task에 굉장히 강함\n",
    "\n",
    "문장만 비교한다고 하면 엔그램으로도 충분\n",
    "하지만 생성에서는 NNLM 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [03. Chapter3. Data Preparation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Ch 03. Data Preparation - 01. AI-Hub 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<앞으로 우리는…>\n",
    "-Machine Learning Exercise\n",
    "Text Preprocessing -> Sequence to Sequence(학습) -> Mini-batch Parallelized Beam Search(추론)\n",
    "-> Transformer(학습) -> Minimum Risk Training(강화학습) -> Dual Supervised Learning(강화학습 보완)\n",
    " \n",
    "<AI Hub>\n",
    "www.aihub.or.kr\n",
    "\n",
    "<한국어-영어 번역(병렬) 말뭉치 AI데이터>\n",
    "-한국어-영어 160만 문장의 번역 말뭉치\n",
    "심지어 문장by문장으로 맵핑돼 있음, 전처리도 다 돼 있음, 토크나이제이션만 하면 됨.\n",
    "\t-문어체 한영 번역 110만 문장\n",
    "\t뉴스 80만, 정부 웹사이트 콘텐츠 10만, 조례 10만, 한국문화 10만\n",
    "\t-구어체 한영 번역 50만 문장\n",
    "\t구어체 40만, 대화체 10만\n",
    "-이외에도 법률, 특허, 일반상식, 한국어 대화, 기계 독해(MRC) 등의 말뭉치 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Ch 03. Data Preparation - 02. 실습 번역 말뭉치 신청 및 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "번역 같은 경우 양방향 코퍼스를 구하기가 어렵다.\n",
    "AIhub 통해 쉽게 구할 수 있다.\n",
    "신청 후 승인까지 며칠 걸림.\n",
    "\n",
    "엑셀 파일을 텍스트 파일로 복사\n",
    "맨 마지막 줄은 한 줄 꼭 띄워줘라\n",
    "엑셀 파일을 텍스트 파일로 복사하면 컬럼 구분은 탭으로 바뀐다. tsv파일이 되는 것.\n",
    "10개 파일을 모두 이러한 방식으로 변환. 노가다.\n",
    "\n",
    "wc -l ./*.txt\n",
    "전체 몇 라인이 있는지 확인할 수 있다\n",
    "\n",
    "wc ./*.txt\n",
    "아무 옵션 없이 치면 여러가지 정보를 볼 수 있다\n",
    "라인 수 / 단어 수 / 캐릭터 수\n",
    "캐릭터 수를 단어 수로 나눠서 파일 당 얼마나 다른 속성을 가지고 있는지도 확인해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: 03. Ch 03. Data Preparation - 03. 실습 데이터 살펴보기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
