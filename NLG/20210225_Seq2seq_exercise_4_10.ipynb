{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Ch 04. Sequence-to-Sequence - 10. 실습 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "어텐션은 nn.Module을 상속받아 선언이 될 건데 해당 클래스는 리니어 레이어를 가지고 있을 것\n",
    "제너레이터 안에도 리니어 레이어와 소프트맥스가 들어있을 것\n",
    "concat layer에도 리니어 레이어 \n",
    "\n",
    "<인코더 투 디코더>\n",
    "인코더의 마지막 타임 스텝의 히든스테이트가 디코더의 이니셜 스테이트가 된다.\n",
    "바이 디렉셔녈 LSTM -> 유니 디렉셔널 LSTM\n",
    "똑같지 않기 떄문에 히든스테이트가 호환이 안 된다.\n",
    "\n",
    "인코더가 레이어 3개라고 하면 디렉션이 두 방향(backward, forward)이므로 총 6개의 히든 스테이트가 나온다\n",
    "디코더는 그냥 레이어 개수 만큼의 히든 스테이트가 나온다.\n",
    "인코더는 히든 사이즈가 반으로 줄어서 선언이 될 것.\n",
    "포워드에 백워드를 갖다붙이는 식으로 맞출 수 있다.\n",
    "그냥 두 개 평균내도 된다.\n",
    "\n",
    "<Training vs Inference추론>\n",
    "트레이닝과 추론이 같지 않다.\n",
    "인코더와 디코더가 각각 다르게 구현된다\n",
    "왼쪽이 트레이닝이라서 티처 포싱, hat이 없음\n",
    "각각 짜줘야 한다.\n",
    "\n",
    "train.py\n",
    "GPU에서 돌려도 며칠 걸린다.\n",
    "굉장히 오래 걸림\n",
    "\n",
    "continu_train.py\n",
    "학습하다가 중간에 꺼질 수도 있는데 학습을 재개해주는 역할\n",
    "\n",
    "translate.py(추론)\n",
    "학습이 모두 끝난 파이토치 세이브된 파일을 가지고, 웨이트 파일 가지고 와서 스탠다드 인풋을 받아서 스탠다드 아웃풋으로 번역 결과를 뱉어주는 역할.\n",
    "\n",
    "우리는 나중에 트랜스포머도 배우게 된다.\n",
    "이때 모델을 시퀀스투시퀀스에서 트랜스포머로 갈아끼우기만 하면 됨.\n",
    "텍스트 분류 때 했던 것과 유사한 프레임워크라고 보면 됨.\n",
    "트랜스포머나 시퀀스투시퀀스 똑같은 인터페이스를 갖고 있다.\n",
    "\n",
    "나중에 강화학습할 때는 트레이너 대신에 RL 트레이너가 들어갈 거다.\n",
    "듀얼러닝할 때는 듀얼 트레이너가 들어간다.\n",
    "그 경우에도 모델은 그대로 두고 트레이너만 갈아 끼우면 다른 방식의 트레이너가 동작한다는 것.\n",
    "이렇게 효율성 최대화하는 방향으로 나아가야 한다.\n",
    "\n",
    "<Training Procedure>\n",
    "트레이닝 : 이터레이션 시작 -> 피드 포워드 -> 로스 계산 -> 역전파 -> 그레디언트 디센트 -> 현재 상태 출력 -> 이터레이션 종료\n",
    "-> 학습 에포크 종료 -> 검증 에포크 시작\n",
    "밸리데이션 : 이터레이션 시작 -> 피드 포워드 -> 로스 계산 -> 현재 상태 출력 -> 이터레이션 종료\n",
    "-> 검증 에포크 종료 ->\n",
    "베스트 로스 여부 체크 -> 모델 저장\n",
    "-> 학습 에포크 시작\n",
    "\n",
    "MLE 트레이닝을 하게 된다면 티피컬한 학습을 하기 위한 절차가 정해져 있다.\n",
    "Boiler Plate라고 하는 학습을 하기 위한 템플릿을 짜놓으면 계속해서 재활용을 할 수 있다.\n",
    "파이토치 이그나이트로 짜서 활용(라이트닝도 있다, 각자 장단점 있음, 다만 이그나이트는 파이토치 공식 라이브러리)\n",
    "\n",
    "모두 이벤트: 이터레이션 시작, 이터레이션 종료, 학습 에포크 종료, 검증 에포크 시작, 검증 에포크 종료, 학습 에포크 시작\n",
    "이벤트에 의해 트리거된 프로세스들로 만들 수 있다.\n",
    "이그나이트 같은 경우에는 이러한 이벤트에 기반해서 이벤트 핸들러를 등록해서(콜백함수를 등록해서) 짤 수 있도록 기능을 제공하고 있음.\n",
    "각각의 이벤트에 대해 핸들러를 등록하는 형태로 이그나이트의 커스텀 엔진을 짜서 사용하고 있다.\n",
    "MLE 트레이닝도 그렇고, 강화학습, 듀얼러닝에서도 마찬가지.\n",
    "이벤트에 기반해서 절차를 진행하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: 11. Ch 04. Sequence-to-Sequence - 11. 실습 Encoder 구현하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
