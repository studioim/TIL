{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Ch 02. Language Modeling - 09. Perplexity and Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perplexity(PPL), 언어 모델을 평가하는 방법\n",
    "PPL과 엔트로피의 관계\n",
    "<Perplexity>\n",
    "문장의 확률에 역수를 취하고 기하평균한 값\n",
    "문장의 확률 값에 -1/n승해준 것\n",
    "확률값은 높을수록 PPL은 낮을수록 좋은 것\n",
    "헷갈리는 단어의 개수도 된다.\n",
    "\n",
    "주사위는 유니폼 디스트리뷰션\n",
    "이보다 flat한 디스트리뷰션은 없다.\n",
    "주사위의 PPL은 6이다\n",
    "PPL이 낮을수록 샤프한 형태\n",
    "\n",
    "<Information and Entropy>\n",
    "-정보 이론에서 엔트로피는 어떤 정보의 불확실성을 나타냄\n",
    "-불확실성은 일어날 것 같은 사건(likely event)의 확률\n",
    "자주 발생하는(일어날 확률이 높은) 사건은 낮은 정보량을 가진다\n",
    "드물게 발생하는(일어날 확률인 낮은) 사건은 높은 정보량을 가진다\n",
    "-불확실성이 높으면 확률이 낮고 정보량은 높다\n",
    "\n",
    "-누구나 다 아는 얘기를 말해봤자 그건 정보가 아니다\n",
    "아무도 모르는 얘기가 정보다\n",
    "\n",
    "-정보량\n",
    "\t-log 때문에, 확률이 0에 가까워질수록 높은 정보량\n",
    "-언어모델 관점\n",
    "흔히 나올 수 없는 문장(확률이 낮은 문장)일수록 더 높은 정보량\n",
    "\n",
    "<엔트로피>\n",
    "엔트로피가 높을수록 플랫한 디스트리뷰션을 가진다\n",
    "낮을수록 샤프한 디스트리뷰션을 가진다\n",
    "\n",
    "<퍼플렉서티>\n",
    "-확률값 역수의 기하평균\n",
    "\n",
    "산술평균은 그냥 1/n, 기하평균은 1/n승\n",
    "\n",
    "<엔트로피와 퍼플렉서티>\n",
    "-PPL은 작을수록 좋은 것\n",
    "PPL 미니마이즈는 결국 크로스엔트로피 미니마이즈하는 것이라고 볼 수도 있는 것\n",
    "-우리는 다음 단어를 예측하려는 건데 단어는 이산적 밸류이니까 크로스 엔트로피 쓴다고 할 수도 있는 것\n",
    "-두 분포 P와 P세타 간의 사이를 미니마이즈할거니까 우리는 크로스 엔트로피 미니마이즈 하는 것\n",
    "-가능도를 최대화할 건데 이는 네거티브로그가능도를 최소화하는 거고, 이는 크로스 엔트로피 미니마이즈와 똑같고 그래서 우리는 크로스 엔트로프 로스를 쓰는 거고\n",
    "등등… 다양하게 해석 가능하다\n",
    "\n",
    "<요약>\n",
    "-우리는 퍼플렉서티 최소화가 목표\n",
    "크로스엔트로피 최소화와 동일\n",
    "NLL 최소화와도 동일\n",
    "-문장의 가능도를 최대화하는 파라미터를 찾고 싶음\n",
    "Ground-truth 확률 분포(실제 사람이 가진 언어 모델)에 언어모델을 근사(approximate)하고 싶음\n",
    "-GT 분포와 LM 분포 사이의 크로스 엔트로피를 구하고 미니마이즈\n",
    "문장의 퍼플렉서티를 미니마이즈\n",
    "\n",
    "차후 뉴럴네트워크를 가지고 언어모델 학습을 할 때 소프트맥스를 써서 크로스 엔트로피를 쓸 것이다\n",
    "그럼 크로스 엔트로피 로스가 나올 것\n",
    "그런데 거기에 그냥 익스퍼넨셜을 취해주면 해당 언어모델의 트레이닝 셋(혹은 밸리데이션 셋)에 대한 퍼플렉서티를 알 수 있는 것이다.\n",
    "크로스엔트로피로스 자체는 와닿는 게 없지만\n",
    "퍼플렉서티가 의미하는 것을 우리는 알고 있다. 헷갈리는 단어의 개수(적어야 하니까 퍼플렉서티는 낮아야 한다)\n",
    "현재 이 언어모델이 매 타임스텝마다 헷갈려 하는 단어가 몇개구나 알 수 있는 것.\n",
    "\n",
    "언어모델을 엔그램이 아니라 뉴럴네트워크로 활용해 학습할 떄 퍼플렉서티를 어떻게 직접적으로 구할 수 있는지\n",
    "왜 크로스 엔트로피를 미니마이즈하는지 말해봤다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Ch 02. Language Modeling - 10. Autoregressive and Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "모든 주제의 원인이라고 볼 수 있다.\n",
    "어려움은 이것 때문에 격게 될 것\n",
    "자연어분야 공부를 하려면 평생 지고 가야할 십자가 같은 존재\n",
    "\n",
    "<Applications>\n",
    "-매니투원\n",
    "-원투매니\n",
    "-매니투매니\n",
    "\n",
    "시퀀스투시퀀스는 매니투원과 원투매니가 결합된 형태\n",
    "굳이 넣자면 원투매니로 봐야 한다는 입장\n",
    "\n",
    "이를 두 가지로도 나눠 볼 수 있다.\n",
    "<Two Approaches>\n",
    "1.Non-autoregressive(Non-generative)\n",
    "-현재 상태가 앞 뒤 상태를 통해 정해지는 경우\n",
    "예. Part of Speech(POS) Tagging, Text Classification\n",
    "-Bidirectional RNN 사용 권장(문장을 처음부터 끝까지 다 봐야 하니까, 굳이 안 쓸 이유가 없음)\n",
    "\n",
    "1.Autoregressive(Generative)\n",
    "-현재 상태가 과거 상태에 의존해 정해지는 경우(과거에서 현재로 방향성이 존재)\n",
    "예, NLG, Machine Translation\n",
    "-원투 매니 케이스 해당\n",
    "-Bidirectional RNN 사용 불가!\n",
    "right to left여도 되고 left to right여도 되지만 그 순서대로 가야 한다\n",
    "앞에 것에 의존적, 독립적이지 않음\n",
    "unidirectional RNN을 사용해야 한다\n",
    "\n",
    "<오토리그레시브>\n",
    "과거 자신의 상태를 참조해 현재 자신의 상태를 업데이트 \n",
    "\n",
    "<Teacher-Forcing>\n",
    "-MLE의 수식상, 정답 xt-1을 RNN의 입력으로 넣어줘야 함\n",
    "\n",
    "<Auto-regressive and Teacher-Forcing>\n",
    "원래 추론할 때 학습하던 방식으로 하지 못한다는 것\n",
    "로스를 구하지 못한다\n",
    "그럼 어떻게 학습할 거냐?\n",
    "그래서 나온 게 티처 포싱\n",
    "오토리그레시브한 속성을 가진 시퀀셜 모델이 학습을 하는 방법\n",
    "\n",
    "<Teacher Forcing>\n",
    "x1 hat이 나왔지만 x1(정답)이 들어간다.\n",
    "여기서부터 문제가 생긴다\n",
    "시퀀스투시퀀스, 강화학습, 듀얼러닝…\n",
    "\n",
    "<고통의 시작: NLG is Auto-regressive Task>\n",
    "-오토 리그레시브 태스크에서는 보통 이전 타임 스텝의 모델 출력을 다음 타임 스텝의 입력으로 넣어 줌\n",
    "이전 타임스텝의 출력에 따라 현재 모델의 스테이트가 바뀌게 될 것\n",
    "하지만 문제는 학습을 이렇게 할 경우 MLE 수식에 다르게 동작한다\n",
    "-적절한 학습을 위해서는 학습 시에는 이전 타임 스텝의 출력 값이 아닌, 실제 정답을 넣어 줌(티처 포싱)\n",
    "-따라서 학습과 추론을 위한 방법이 다르게 돼 여러가지 문제가 발생\n",
    "학습을 위한 코드와 추론을 위한 코드를 따로 짜야 함\n",
    "학습과 추론 방법의 괴리(discrepancy)가 발생해 성능이 저하될 수 있음\n",
    "\t-그래도 딥러닝 안 쓰는 방법보다는 이게 훨씬 잘 된다\n",
    "\t-성능 저하 여지가 있으나 걱정할 필요는 없다는 것\n",
    "\n",
    "티처포싱을 활용해서 오토리그레시브한 태스크에서 학습을 할 수 있게 만들었지만\n",
    "그로 인한 부작용들이 추가적으로 생기게 된다.\n",
    "사실 부작용 무시할 만큼 티처포싱해도 추론 잘 된다. \n",
    "그래도 성능을 더 올릴 여지가 있다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: 11. Ch 02. Language Modeling - 11. 정리하며"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
