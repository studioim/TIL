{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [02. Chapter2. Language Modeling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Ch 02. Language Modeling - 01. 들어가며"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-언어모델은 문장의 확률을 나타낸 모델\n",
    "문장 자체의 출현 확률을 예측하거나\n",
    "이전 단어들이 주어졌을 때 다음 단어를 예측하기 위한 모델\n",
    "\n",
    "예)\n",
    "\"버스정류장에서 방금 버스를 사랑해\"라는 말은 방금 뱉었기 때문에 세상에 존재하는 말\n",
    "다만 확률이 극히 낮을 뿐\n",
    "\n",
    "-우리의 머리 속에는 단어와 단어 사이의 확률이 우리도 모르게 학습돼 있음\n",
    "대화를 하다가 정확하게 듣지 못해도 대화에 지장이 없음\n",
    "-많은 문장들을 수집해 단어와 단어 사이의 출현 빈도를 세어 확률을 계산\n",
    "-궁극적인 목표는 우리가 일상 생활에서 사용하는 언어의 문장 분포를 정확하게 모델링하는 것\n",
    "특정 분야(도메인)의 문장의 분포를 파악하기 위해서 해당 분야의 말뭉치를 수집하기도\n",
    "\n",
    "<Again, Korean is Hell>\n",
    "-단어와 단어 사이의 확률을 계산하는데 불리하게 작용\n",
    "한국어는 교착어\n",
    "1)단어의 어순이 중요하지 않기 떄문에\n",
    "2)또는 생략 가능하기 떄문에\n",
    "\n",
    "-예)나는 학교에 갑니다 버스를 타고\n",
    "나는 버스를 타고 학교에 갑니다\n",
    "버스를 타고 나는 학교에 갑니다\n",
    "(나는) 버스를 타고 학교에 갑니다\n",
    "\n",
    "-확률이 퍼지는 현상\n",
    "'타고' 다음에 나타날 수 있는 단어들은 '.', '학교에', '나는' 3개이기 떄문\n",
    "-접사를 따로 분리해주지 않으면 어휘의 수가 기하급수적으로 늘어나 희소성이 더욱 높아짐\n",
    "\n",
    "<Applications>\n",
    "-NLG\n",
    "speech Recognition 음성 인식\n",
    "Machine Translation 기계 번역\n",
    "Optical Character Recognition(OCR)\n",
    "Other NLG Tasks(뉴스 기사 생성, 챗봇 등)\n",
    "Other…(검색어 자동 완성 등)\n",
    "\n",
    "<Automatic Speech Recognition(ASR)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Ch 02. Language Modeling - 02. 언어모델 수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Chain Rule> 사슬법칙\n",
    "We can convert joint probability to conditional probability\n",
    "결합확률과 조건부 확률\n",
    "\n",
    "BOS: Beginning of Sentence\n",
    "EOS: End of Sentence\n",
    "\n",
    "<USING Language Model>\n",
    "-Pick better(fluent) sentence.\n",
    "더 유창한 문장을 선택\n",
    "-Predict next word given previous words.\n",
    "단어들이 주어졌을 떄 다음 단어 예측\n",
    "\n",
    "<요약>\n",
    "-언어모델은 주어진 코퍼스 문장들의 가능도를 최대화하는 파라미터를 찾아내, 주어진 코퍼스를 기반으로 언어의 분포를 학습한다.\n",
    "즉 코퍼스 기반으로 문장들에 대한 확률 분포 함수를 근사(approx)한다\n",
    "-문장의 확률은 단어가 주어졌을 때, 다음 단어를 예측하는 확률을 차례대로 곱한 것과 같다\n",
    "-따라서 언어 모델링은 주어진 단어가 있을 때, 다음 단어의 가능도를 최대화하는 파라미터를 찾는 과정이라고 볼 수 있다.\n",
    "주어진 단어들이 있을 때, 다음 단어에 대한 확률 분포 함수를 근사하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Ch 02. Language Modeling - 03. n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<좋은 모델은 무엇인가?>\n",
    "-Generalization일반화\n",
    "Training(seen) data를 통해 test(unseen) data에 대해 훌륭한 예측을 할 수 있는가?\n",
    "일반화를 잘하는 모델이 좋은 모델\n",
    "\n",
    "-만약 모든 경우의 수에 대해 학습 데이터를 모을 수 있다면 테이블 look-up으로 모든 문제를 풀 수 있을 것(해시 테이블 하나 만들어서)\n",
    "하지만 그것은 불가능하므로 일반화 능력이 중요\n",
    "\n",
    "<Count based Approximation>\n",
    "가장 쉬운 방법\n",
    "코퍼스를 엄청 나게 모아\n",
    "\n",
    "<Problem of Count based Approximation>\n",
    "-특정 워드 시퀀스가 없으면 어떻게 하나?\n",
    "분자가 0이 되면 등장확률은 0\n",
    "이를 보완할 수 있는 방법이 필요\n",
    "\n",
    "<Apply Markov Assumption> 마코프 가정\n",
    "-Approxiate with counting only previous k tokens.\n",
    "앞에 있는 모든 단어들을 다 볼 필요 없이 앞에 k개만 보겠다\n",
    "<이를 센텐스 레벨로 확장하면>\n",
    "이제 우리는 더 많은 단어 시퀀스들을 커버할 수 있다.\n",
    "트레이닝 코퍼스 내에서 보지 못한 단어 시퀀스일지라도.\n",
    "\n",
    "<n-gram>\n",
    "n = k + 1\n",
    "unigram(k=0), bi-gram(k=1), tri-gram(k=2)\n",
    "바이그램은 앞에 한 개만 보겠다는 것\n",
    "\n",
    "-n이 커질수록 오히려 확률이 정확하게 표현되는데 어려움\n",
    "작은 n을 쓰면 마코프 가정이 강하게 들어가는 것(왜곡이 심해지는 것)\n",
    "n이 크면 코퍼스에 없어서 문제가 될 수 있음\n",
    "적절한 n을 사용하자\n",
    "-보통은 트라이그램을 가장 많이 사용\n",
    "-코퍼스의 양이 많을 때는 포그램 사용하기도\n",
    "언어모델의 성능은 크게 오르지 않는데 반해,\n",
    "단어 조합의 경우의 수는 exponential하게 증가하므로 효율성이 없음\n",
    "포그램해도 성능 많이 안 오름\n",
    "\n",
    "<n그램 학습시키는 모델>\n",
    "-SRILM\n",
    "\n",
    "<요약>\n",
    "-확률값을 근사하는 가장 간단한 방법은 코퍼스에서 빈도를 세는 것\n",
    "하지만 복잡한 문장일수록 코퍼스에서 출현 빈도가 낮아 부정확한 근사가 이루어질 것\n",
    "일반화를 제대로 못 한다는 의미\n",
    "-따라서 마코프 가정을 도입해 확률값을 근사하자\n",
    "이제 학습 코퍼스에서 보지 못한 문장에 대해서도 확률값을 구할 수 있다.\n",
    "n의 크기가 중요(n = 3~4가 적당)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Ch 02. Language Modeling - 04. Smoothing and Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "긴 문장의 확률 값을 알기 위해 체인룰로 쪼개서 곱으로 이전 단어들의 곱으로 나타냈었다\n",
    "이때 하나라도 0이 되면 전체가 0\n",
    "0이 되는 것을 막기 위해 마코프 가정을 도입해서\n",
    "앞에 k개만 보겠다고 한 것\n",
    "마코프 가정을 통해 일반화를 한 것\n",
    "\n",
    "<스무딩>\n",
    "-마코프 가정을 도입했지만 여전히 문제는 남아 있음\n",
    "-트레이닝 코퍼스에 없는 언씬 워드 시퀀스의 확률은 0?\n",
    "-언씬 워드 시퀀스에 대한 대처\n",
    "스무딩 또는 디스카운팅\n",
    "-popular algorithm\n",
    "Modified Kneser-Ney Discounting(KN 디스카운팅)\n",
    "\n",
    "SRILM 패키지에 다 구현돼 있고\n",
    "뉴럴 네트워크 사용하면 이러한 문제들은 어느정도 해결됨.\n",
    "\n",
    "<Add One Smoothing>\n",
    "To prevent count becomes zero\n",
    "카운트가 원래 0부터 시작했으면 1부터 시작하도록 하는 것\n",
    "\n",
    "<Generaliation of Add One Smoothing>\n",
    "Add-k Smoothing\n",
    "<Kneser-Ney Discounting>네저네이 디스카운팅\n",
    "-In this lecture,\n",
    "C(learning) > C(laptop)\n",
    "Because of “deep learning”, “machine learning”\n",
    "\n",
    "-다양한 단어 뒤에서 나타나는 단어일수록\n",
    "언씬 워드 시퀀스에 등장할 확률이 높지 않을까?\n",
    "앞에 등장한 단어의 종류가 다양할수록 해당 확률이 높을 것 같음\n",
    "이 같은 아이디어에서 네저네이 디스카운팅 시작\n",
    "\n",
    "<요약>\n",
    "-마코프 가정\n",
    "카운트 기반의 approximation\n",
    "긴 워드 시퀀스는 학습 코퍼스에 존재하지 않을 수 있음\n",
    "\t-확률 값이 0으로 맵핑\n",
    "마코프 가정을 통해 근거리의 단어만 고려\n",
    "\n",
    "-스무딩 앤 디스카운팅\n",
    "마코프 가정을 통해서도 여전히 확률 값이 0이 될 수 있음\n",
    "스무딩 또는 디스카운팅을 통해 현상을 완화\n",
    "여전히 언씬 워드 시퀀스에 대한 대처는 미흡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
