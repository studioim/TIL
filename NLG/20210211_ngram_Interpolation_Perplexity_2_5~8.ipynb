{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Ch 02. Language Modeling - 05. Interpolation and Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram의 부족한 generalization 방법,\n",
    "unseen word sequence를 다루기 위한 방법들\n",
    "\n",
    "<Interpolation 보간법>\n",
    "점과 점 사이의 비어 있는 부분을 채워넣는 것\n",
    "-다른 언어 모델을 리니어하게 일정 비율(람다)로 섞는 것\n",
    "\n",
    "-general domain LM(대량) + domain specific LM(소량)\n",
    "= general domain에서 잘 동작하는 domain adapted LM\n",
    "\n",
    "도메인은 100만 문장 모으면 보통 많이 모으는 것\n",
    "\n",
    "-예)\n",
    "의료 도메인 ASR(Automatic Speech Recognition), MT(Machine Translation) system\n",
    "법률 도메인 ASR, MT system\n",
    "특허 도메인 ASR, MT system\n",
    "Interpolated된 새로운 언어모델을 얻을 수 있다.\n",
    "\n",
    "-그냥 도메인 specific corpus로 LM을 만들면 장땡 아닌가?\n",
    "그럼 언씬 워드 시퀀스가 너무 많을 것 같은데?\n",
    "\n",
    "-그냥 전체 코퍼스를 합쳐서 LM을 만들면 장땡 아닌가?\n",
    "도메인 스피시픽 코퍼스의 양이 너무 적어서 반영이 안 될 수도?\n",
    "의료인 대화는 일반인 대화와 다를텐데\n",
    "\n",
    "-보간법에서 비율(ratio, 람다)을 조절해 중요도(weight)를 조절\n",
    "명시적(explicit)으로 섞을 수 있다\n",
    "General domain test set, Domain specific test set 모두에서 좋은 성능을 찾는 하이퍼 파라미터 람다를 찾아야 한다.\n",
    "\n",
    "<보간법 예>\n",
    "-“준비 된 진정제 를 투여 합 시다”\n",
    "\n",
    "음성인식에서 '준비된' 다음 '진정제'가 잘 안 들렸으면 사나이로 가고 그런다.\n",
    "보간법을 통해서 '준비된 진정제'가 나올 가능성을 높여줄 수 있다.\n",
    "보간법의 기본 개념을 조금 더 응용하면 백오프를 생각해 볼 수 있다.\n",
    "\n",
    "<Back-off>\n",
    "-희소성에 대처하는 방법\n",
    "마코프 가정처럼 n을 점점 줄여가면?\n",
    "\t-조건부 확률에서 조건부 워드 시퀀스를 줄여가면,\n",
    "\t-Unknown(UNK) word가 없다면 언젠가는 확률을 구할 수 있다!\n",
    "\n",
    "트라이그램, 바이그램, 유니그램까지 Interpolation 해버리는 것\n",
    "\n",
    "<Back-off Example>\n",
    "\n",
    "<요약>\n",
    "-백오프를 통해 확률 값이 0이 되는 현상은 방지할 수 있음 - OoV 제외\n",
    "하지만 언씬 워드 시퀀스를 위해 백오프를 거치는 순간 확률 값이 매우 낮아져 버림\n",
    "여전히 음성인식(ASR) 등의 활용에서 어려움이 남음\n",
    "\n",
    "-전통적인 방식의 NLP에서는 단어를 discrete symbol로 보기 떄문에 문제 발생\n",
    "Exact matching에 대해서만 count를 하여, 확률 값을 근사\n",
    "다양한 방법을 통해 문제를 완화하려 하지만 근본 해결책은 아님\n",
    "\t-마코프 가정 Markv Assumption\n",
    "\t-스무딩과 디스카운팅 Smoothing and Discounting\n",
    "\t-보간법과 백오프 Interpolation and Back-off\n",
    "언어모델은 NLG가 아닌 곳에서는 충분히 좋은 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Ch 02. Language Modeling - 06. Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "언어모델을 평가하는 방법\n",
    "\n",
    "<How to evaluate LM>\n",
    "-Test set\n",
    "1)나는 학교에 갑니다\n",
    "2)나는 학교를 갑니다\n",
    "\n",
    "-Intrinsic evaluation(정성평가)\n",
    "사람이 직접하는 정성평가\n",
    "정확함\n",
    "시간과 비용이 많이 들어감\n",
    "\n",
    "-Extrinsic evaluation(정량평가)\n",
    "컴퓨터가 주어진 테스트셋에 대해서 자동으로 수행\n",
    "시간과 비용을 아낄 수 있음\n",
    "Intrinsic evaluation과 비슷할수록 좋은 방법\n",
    "Perplexity는 정량평가를 수행(스코어링)하기 위한 수식이라고 보면 됨.\n",
    "\n",
    "<좋은 언어모델이란?>\n",
    "-실제 사용하는 언어의 분포를 가장 잘 근사한 모델\n",
    "실제 사용하는 언어 -> 테스트 시의 입력 문장들\n",
    "분포를 잘 근사 -> 문장의 가능도가 높을 것\n",
    "-잘 정의된 테스트셋의 문장에 대해서 높은 확률을 반환하는 언어모델이 좋은 모델\n",
    "\n",
    "<평가>\n",
    "-Perplexity(PPL)\n",
    "테스트 문장에 대해서 언어모델을 이용해 확률(가능도)을 구하고\n",
    "PPL 수식에 넣어 언어모델의 성능 측정\n",
    "\t-문장의 확률을 길이에 대해서 normalization(기하평균)\n",
    "\n",
    "확률은 높은 게 좋으니까 PPL은 작은 게 좋음(확률의 역수가 사용됐으므로)\n",
    "\n",
    "-Chain rule에 의해서\n",
    "\n",
    "<Perplexity>\n",
    "-테스트 문장에 대해서 확률을 높게 반환할수록 좋은 언어모델\n",
    "-테스트 문장에 대한 PPL이 작을수록 좋은 언어모델\n",
    "\n",
    "-주사위를 던져 봅시다\n",
    "1부터 6까지의 6개의 숫자로 이루어진 수열\n",
    "6개의 숫자의 출현 확률은 모두 같다\n",
    "uniform distribution\n",
    "PPL(x1, …, xn) = 6\n",
    "\n",
    "주사위에서 숫자를 예측할 떄 6개 중에서 동등하게 헷갈리고 있다.\n",
    "조금이라도 의심가는 쪽으로 예측할 수는 없다.\n",
    "PPL이라고 하는 것은 얼마 중에서 헷갈리고 있냐는 것을 나타낸다\n",
    "\n",
    "-PPL을 해석하는 방법\n",
    "주사위 PPL: 매 타임 스텝 가능한 가지 수인 6\n",
    "뻗어나갈 수 있는 브랜치(가지)의 숫자를 의미\n",
    "타임 스텝 별 평균 브랜치의 수\n",
    "PPL이 낮을수록 확률 분포가 Sharp하다\n",
    "PPL이 높을수록 확률분포가 Flat하다\n",
    "\n",
    "<요약>\n",
    "-좋은 언어모델:\n",
    "잘 정의된 테스트셋 문장에 대해서 높은 확률(=낮은 PPL)을 갖는 모델\n",
    "\n",
    "-Perplexity(PPL)\n",
    "Lower is better\n",
    "확률의 역수에 문장 길이로 기하평균\n",
    "매 타임 스텝마다 평균적으로 헷갈리고 (no clue) 있는 단어의 수\n",
    "헷갈리는 게 없어야 하므로 PPL은 낮을수록 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Ch 02. Language Modeling - 07. n-gram 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<n-gram 결론>\n",
    "n-gram은 딥러닝 이전에 수십년을 지배해온 방법\n",
    "가장 간단하게 언어모델을 구현할 수 있는 방법\n",
    "그러나 마코프 가정, 스무딩, 백오프 등 배우는 이유가 애초에 일반화가 안 되기 때문\n",
    "unseen word sequence에 대처가 안 된다.\n",
    "정확하게 매칭이 됐을 때 카운트가 된다. 대상이 있을 때\n",
    "단어 자체를 이산적 심볼로 취급하기 때문\n",
    "\n",
    "-Pros\n",
    "Scalable: 쉽게 (large vocabulary 등의) 대형 시스템에 적용 가능\n",
    "n-gram 훈련 및 추론 방식이 굉장히 쉽고 간편\n",
    "\n",
    "-Cons\n",
    "Poor generalization: 등장하지 않은 단어 조합에 대처 미흡\n",
    "\t-단어를 discrete symbol로 취급\n",
    "\t-따라서 비슷한 단어에 대한 확률을 이용(leverage, exploit)하지 못함\n",
    "\t-Smoothing과 Back-off 방식을 통해서 단점을 보완하려 했으나, 근본적인 해결책이 아님\n",
    "Poor with long dependency: 멀리 있는 단어에 대해 대처 불가\n",
    "n이 커질수록 용량도 커짐\n",
    "\n",
    "-실제로 애플리케이션 적용(ASR, SMT)에 있어서 큰 과제\n",
    "\n",
    "-> 딥러닝 시대에 오면서 단어를 이산적 심볼로 취급하지 않고 임베딩 레이어를 거쳐서 연속적 벡터로 변환하게 된다.\n",
    "보지 못한 것에 대해서도 유사도를 구할 수 있다.\n",
    "일반화가 잘 됨! 기존 문제 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Ch 02. Language Modeling - 08. RNN을 활용한 LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural Language Model\n",
    "-Resolve Sparsity(희소성)\n",
    "Training set: 고양이는 좋은 반려동물입니다.\n",
    "Test set: 강아지는 훌륭한 애완동물입니다.(Unseen Word Sequence)\n",
    "\n",
    "-Because we know(and we can approximate that)\n",
    "고양이, 강아지 비슷\n",
    "좋은, 훌륭한 비슷\n",
    "반려동물, 애완동물 비슷\n",
    "-But n-gram CANNOT, because words are discrete symbols.\n",
    "뉴럴 네트워크는 더 이상 이산적 심볼로 취급하지 않아 이 문제를 해결할 수 있다.\n",
    "\n",
    "<Neural Langauge Model>\n",
    "-Find parameter that maximize likelihood for given training corpus.\n",
    "-Take a step of gradient descent to minimize negative log-likelihood.\n",
    "\n",
    "<Loss Function of NNLM(Neural Network Language Model)>\n",
    "-Find theta that minimize negative log-likelihood.\n",
    "-Find theta that minimize cross entropy with ground-truth probability distribution.\n",
    "Cross Entropy Loss 쓰면 된다.\n",
    "\n",
    "파이토치에서 구현할 땐 소프트맥스 더하기 크로스엔트로피로스인데\n",
    "이게 파이토치에서는 다르게 구현될 수 있다.\n",
    "로그 소프트맥스를 쓴 다음에 NLLLoss를 써도 된다.\n",
    "아무거나 해도 됨. 속도상 차이가 있다고 하지만 실제로는 크리티컬한 차이는 없음.\n",
    "\n",
    "<요약>\n",
    "-n-gram(previous method)\n",
    "단어를 discrete symbol로 취급\n",
    "\t-Exact matching에 대해서만 카운트\n",
    "따라서 generalization issue 발생\n",
    "\t-Markov Assumption 도입(n-gram)\n",
    "\t-Smoothing & Discounting\n",
    "\t-Interpolation & Back-off\n",
    "\t-Unseen word sequence에 대한 대처 미흡\n",
    "빠른 연산 & 쉽고 직관적\n",
    "\t-단순한 look-up table 방식\n",
    "\t-문장 fluency 비교 task에서는 괜찮음\n",
    "\n",
    "-Neural Network Language Model\n",
    "\t워드 임베딩을 통해 unseen sequence에 대해 대처 가능\n",
    "-Generation taks에서 특히 강점\n",
    "\t-마코프 가정이 안 들어감\n",
    "\t-long dependency에 대해 대처 가능\n",
    "\t-워드 임베딩 레이어를 통해 단어 자체를 연속적 변수로 다루다보니 일반화 잘 됨\n",
    "\t-뭐라도 확률값 자체는 뱉어낸다\n",
    "-연산량 많음(feed forward 연산)\n",
    "\t-행렬곱 엄청 해야 함. 연산량 많음\n",
    "\t-해석(XAI) 난이도 증가\n",
    "\t어떤 문장이 잘 인식, 대처가 안 될 때 어떻게 해결할 건지 해결이 잘 안 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
