{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. Ch 08. Recurrent Neural Networks - 01. RNN 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "기존에 다뤄왔던 뉴럴 네트워크와는 다른 구조\n",
    "-Previous Methods\n",
    "예. Tabular Data, Image\n",
    "x -> Model -> y\n",
    "y = f(x;세타)\n",
    "\n",
    "-Recurrent Neural Networks\n",
    "예. Sequential Data, Time-series\n",
    "xt -> 모델 - ht -> xt\n",
    "출력이 입력값으로 들어간다\n",
    "ht = f(xt, ht-1;세타)\n",
    "->y\n",
    "입력 받을 때 이전 타임스텝의 출력과 현재 타임 스텝의 입력을 동시에 받는다\n",
    "따로 y라는 게 존재하지 않는다고 생각하는 게 좋음\n",
    "\n",
    "<Sequential Data vs Time Series>\n",
    "-Time-stamp의 유무에 따른 차이\n",
    "\t-시퀀셜 데이터는 데이터의 순서 정보가 매우 중요함\n",
    "\t예. 텍스트 문장: 단어의 순서\n",
    "\t-추가로 시계열 데이터는 해당 데이터가 발생한 시각 정보가 매우 중요함\n",
    "\t예. 주식 데이터: 가격의 순서 및 발생 시점\n",
    "-Time Series Data(시계열 데이터)\n",
    "주식 데이터(거래 시점)\n",
    "센서 데이터\n",
    "-Sequential Data\n",
    "텍스트\n",
    "(샘플링 주기가 일정한) 영상/음성\n",
    "\n",
    "RNN은 타임시리즈 데이터를 위한 아키텍처라고 볼 수 없다.\n",
    "타임스탬프를 바로 받아들일 수 있는 아키텍처는 아니고 시퀀셜 데이터를 위한 아케틱처에 오히려 맞는 편이긴 함\n",
    "우리가 시계열 데이터를 모은 다음에 우리가 인터벌을 일정하게 바꿔준다거나 샘플링 레이트를 일정하게 바꿔주는 전처리를 거치게 돼서\n",
    "시퀀셜 데이터로 만들 수 있게 된다면 RNN에 넣을 수 있다.\n",
    "\n",
    "기존의 FC모듈, FC레이어나 CNN같은 경우 애초 순서 정보, 이전 데이터에 기반한 현재 데이터 처리가 불가능한 반면에,\n",
    "RNN은 순서 정보나 이전 데이터에 기반한 현재 데이터의 처리가 가능하다는 특징을 갖고 있다.\n",
    "\n",
    "<RNN, 더 이상 어려워하지 말자!>\n",
    "-시간(순서) 개념이 추가되니 어려움을 느낄 수 있음\n",
    "Generation task가 아니면 사실 시간 개념을 구현에 넣을 필요가 없음\n",
    "일단 입/출력 텐서의 모양만 알아도 이미 반은 해결\n",
    "-Back-propagation이 어렵다?\n",
    "수식을 몰라도 기본 원리만 기억하자\n",
    "-스텝바이스텝으로 접근하면 많은 variation에 대해서도 해결됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. Ch 08. Recurrent Neural Networks - 02. RNN Step-by-Step 들여다보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "현재 타임스텝의 입력과 이전 타임스텝의 출력을 입력으로 함께 받는다.\n",
    "y라는 것이 따로 없음\n",
    "계속 뱅글뱅글 돈다.\n",
    "tanh를 논리니어액티베이션 펑션으로 쓴다.\n",
    "\n",
    "yt hat = ht = f(xt, ht-1;세타)\n",
    "=tanh(Wih xt + bis) + (Whh ht-1 + bhh)\n",
    "where 세타 = {Wih, bih, Whh, bhh}\n",
    "= tanh(W[xt, ht-1] # 바이어스는 보통 많이 생략함\n",
    "\n",
    "세타는 역전파되기 전까지 모두 동일\n",
    "세타는 x와 h가 들어오는 거에 대해서 시간에 관계 없이 항상 잘 동작해야 한다고 할 수 있다\n",
    "\n",
    "<How it works>\n",
    "L(세타) = 시그마(t=1, T) ||yt hat - yt||\n",
    "\n",
    "<Zoom In : Input Tensor>\n",
    "|xt| = (batch_size, 1, input_size) *n -> |X| = (batch_size, n, input_size) where X = {x1,x2,…,xn}\n",
    "단어 -> 문장\n",
    "\n",
    "이미지는 (N, C, H, W)의 4차원 텐서 사용\n",
    "문장은 (N, Length, Dimension)의 3차원 텐서 사용\n",
    "\n",
    "생성 태스크가 아닌 경우에는 오른쪽처럼 한번에 데이터를 넣어 준다.\n",
    "\n",
    "<Zoom In : Hidden Tensor>\n",
    "히든 스테이트 텐서\n",
    "출력이라고도 볼 수 있다\n",
    "\n",
    "|ht| = (batch_size, hidden_size) *n -> |h1:n| = (Batch_size, n, hidden_size) where h1:n = [h1;h2;…;hn]\n",
    "입력에 전체 시간이 한 번에 들어가면 한 번에 다 출력으로 나온다\n",
    "\n",
    "<Multi-layered RNN>\n",
    "앞전까지는 싱글 레이어 RNN\n",
    "어떻게 구현할지 고민할 필요 없다. 파이토치 파라미터(number of layers)로 지정해주면 된다.\n",
    "마지막 레이어의 히든 스테이트가 y hat이 된다.\n",
    "세타는 타임스텝에 따라선 모두 같지만 층에 대해선 다르다.\n",
    "각 층 마다 다른 파라미터\n",
    "\n",
    "<Zoom In : Output Tensor>\n",
    "|h1:n| = (batch_size, n, hidden_size)\n",
    "\n",
    "<Zoom In : Hidden State Tensor>\n",
    "|ht| = (#layers, batch_size, hidden_size)\n",
    "\n",
    "<Bidirectional Multi-layered RNN>\n",
    "RNN을 두 방향으로도 만들 수 있다. 정방향, 역방향.\n",
    "통째로 입력해주고 통째로 출력을 뱉는 문제 형태에서는 가장 많이 사용.\n",
    "\n",
    "<Zoom In : Output Tensor>\n",
    "마지막 레이어의 정방향의 각 타임스텝별 히든스테이트들과 마지막 레이어의 역방향의 각 타임스텝별 히든스테이트들이 모두 모이게 된다.\n",
    "|h1:n| = (batch_size, n, hidden_size X #directions), directions=2\n",
    "\n",
    "<Zoom In : Hidden State Tensor>\n",
    "|ht| = (#directions X #layers, batch_size, hidden_size), directions=2\n",
    "t라고 해놨기 때문에 시간 개념이 안 들어 있음. 이 텐서 안에는\n",
    "엄청 중요하지는 않다.\n",
    "바이디렉셔널 멀티레이어 RNN을 하게 되면 전체 타임스텝으로 입력, 전체 타임스텝으로 출력되기 때문에\n",
    "다음 입력 받기 위한 작업들을 잘 하지 않는 경우가 대부분.\n",
    "그래도 구조는 알아야.\n",
    "\n",
    "<You can refer documents, anytime>\n",
    "파이토치 다큐먼트 잘 돼 있다.\n",
    "Input shape\n",
    "Output shape\n",
    "\n",
    "<요약>\n",
    "-Single layer RNN에서 hidden state는 곧 output이다.\n",
    "-Multi-layered RNN에서\n",
    "\t-Output은 마지막 레이어의 모든 타임스텝의 히든스테이트이다.\n",
    "\t-히든 스테이트는 마지막 타임 스텝의 모든 레이어의 히든 스테이트이다.\n",
    "-Bi-directional RNN에서\n",
    "\t-Output은 hidden state의 2배가 된다\n",
    "\t-히든 스테이트는 레이어의 개수가 2배가 된다\n",
    "기본 원리만 잘 이해하고 있어도 어렵지 않게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. Ch 08. Recurrent Neural Networks - 03. RNN 활용 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Applications>\n",
    "-Many(입력) to One(출력)/Text Classification(문장 받은 다음 어떤 클래스에 속하는지 결과값으로)\n",
    "-One to Many/NLG, Machine Translation\n",
    "-Many to Many/POS Tagging(형태소분석), MRC(Machine Reading Comprehesion)\n",
    "\n",
    "<Two Approaches>\n",
    "1.Non-autoregressive(Non-generative)\n",
    "-현재 상태가 앞/뒤 상태를 통해 정해지는 경우\n",
    "\t-예.Part of Speech(POS) Tagging, Text Classification\n",
    "-Bidirectional RNN 사용 권장(문장의 처음과 끝을 다 받은 상태이므로)\n",
    "문장 전체를 다 받거나 시퀀스를 다 받는 경우\n",
    "2.Autoregressive(Generative)\n",
    "-현재 상태가 과거 상태에 의존해 정해지는 경우\n",
    "이전 타임스텝의 정보를 가지고 현재가 정해진다.\n",
    "\t-예. NLG, 기계 번역\n",
    "\t이전에 단어가 있고 이에 따라 현재에는 어떤 단어를 뱉어야 하는지를 고민하는 태스크\n",
    "-One-to-Many case 해당\n",
    "-Bidirectional RNN 사용 불가!\n",
    "미래의 타임스텝에 대한 입력은 아직 주어지지 않음.\n",
    "예. 오전 주식 데이터만 주고 오후 것을 예측하라고 하면 Bidirectional RNN 쓸 수 없다.\n",
    "예. 문장도 마찬가지\n",
    "\n",
    "<Many to Many>\n",
    "예. POS Tagging 형태소 분석\n",
    "\n",
    "<Many to One>\n",
    "예. 텍스트 분류\n",
    "\n",
    "<One to Many>\n",
    "가장 어려운 태스크\n",
    "예. NLG\n",
    "\n",
    "<One to Many(시퀀스투시퀀스)>\n",
    "예. 기계번역\n",
    "Encoder -> Decoder\n",
    "Many to one / One to Many\n",
    "\n",
    "시퀀스투시퀀스\n",
    "시퀀스를 한 점으로, 점을 다시 시퀀스로 바꿔내는 형태\n",
    "\n",
    "-> 인코더의 마지막 히든스테이트를 디코더의 이니셜 스테이트로 넣어주면 이를 바탕으로 문장 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. Ch 08. Recurrent Neural Networks - 04. RNN에서의 Back-propagation (BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Back Propagation Through Time(BPTT)\n",
    "<Back Propagation with Multiple Forwarding Paths>\n",
    "\n",
    "<Back Propagation in RNN>\n",
    "-Many to One\n",
    "타입스텝마다의 세타로 미분해준 그레디언트를 모두 더한다\n",
    "타임스텝에 따라 그레디언트 크기가 바뀜\n",
    "타임스텝이 길어질수록 그레디언트 크기가 커짐\n",
    "너무 긴 시퀀스의 경우에는 그레디언트가 폭발하기도(그레디언트 익스플로딩)\n",
    "\n",
    "-Many to Many\n",
    "한 타임 스텝 안에서도 두 개의 피드포워드 경로가 존재\n",
    "이를 다 더해야 함.\n",
    "\n",
    "-Many to Many with Multi-layered RNN\n",
    "더 다양하게 다 더해진다\n",
    "\n",
    "<TanH in Vanilla RNN> ",
    "여러개의 깊은 레이어가 쌓인 형태\n",
    "마디마다 tanH가 쓰이고 있다\n",
    "깊은 네트워크에서 발생하는 기울기 소멸이 발생할 수밖에 없다.\n",
    "tanH는 -무한대에서는 -1, +무한대에서는 +1\n",
    "tanH가 0일 때 기울기의 값은 1이 된다\n",
    "tanH에 들어가는 값이 0일 때를 제외하고는 모든 그레디언트가 1보다 작게 된다.\n",
    "계속해서 역전파가 돼서 tanH에 그레디언트가 곱해지게 된다.\n",
    "점점 작아지게 되는 효과가 생김\n",
    "\n",
    "<Gradient Vanishing>\n",
    "Hard to train LONG sequence\n",
    "길이가 길어질수록 역전파가 잘 안되는 문제 발생\n",
    "그레디언트 전달이 잘 안 됨\n",
    "긴 시퀀스의 데이터들은 잘 학습이 되지 않는 문제 발생\n",
    "오래된 데이터들은 점점 신경쓰지 않게 됨 -> 그래서 최근에 것만 기억한다\n",
    "\n",
    "<요약>\n",
    "-여러 경로로 feed-froward될 경우, 역전파할 때\n",
    "최종 그레디언트 값은 각 경로의 그레디언트들의 총 합이 된다.\n",
    "(이는 다른 뉴럴네트워크에도 모두 적용되는 룰)\n",
    "-RNN의 경우에도 각 타임 스텝에 따라 피드포워드되므로\n",
    "각 경로로부터 전달돼 온 그레디언트들이 더해진다.\n",
    "-따라서 타임 스텝의 개수만큼 레이어가 깊어진 것과 마찬가지이므로\n",
    "tanh를 활성함수(논리니어 액티베이션 펑션)로 사용하는 RNN은 그레디언트 배니싱이 발생한다.\n",
    "그래서 RNN은 긴 시퀀스에서는 정작 잘 동작하지 않는다.\n",
    "이 문제 해결은 LSTM이라는 아키텍처를 활용해 해결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. Ch 08. Recurrent Neural Networks - 05. 수식 BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT, 강의 참고\n",
    "<Many to One>\n",
    "<Many to Many>\n",
    "매니투원이 여러개 있는것처럼 미분하면 됨.\n",
    "<그레디언트 배니싱>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. Ch 08. Recurrent Neural Networks - 06. Long-Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<그레디언트 배니싱 in Vanilla RNN>\n",
    "-RNN 내부에는 tanh가 있으므로, 타임 스텝이 길어짐에 따라\n",
    "그레디언트 배니싱이 발생함\n",
    "\t-따라서 긴 시퀀스는 학습이 어려움\n",
    "\n",
    "<Gate using sigmoid>\n",
    "-시그모이드는 0과 1사이의 값을 반환하므로\n",
    "시그모이드를 곱하면 마치 문을 열고 닫는 듯한 효과를 낼 수 있음\n",
    "LSTM의 기본 아이디어\n",
    "\n",
    "<Long Short Term Memory(LSTM>\n",
    "이전 타임스텝의 히든 스테이트 말고도 이전 타임스텝의 셀 스테이트를 받아 내보낸다.\n",
    "셀 스테이트는 또 하나의 히든 스테이트라고 할 수 있다.\n",
    "셀 스테이트는 그레디언트 소멸을 해결하는 데 중요한 역할을 한다.\n",
    " \n",
    "LSTM 내부엔 게이트가 많다.\n",
    "게이트의 어떤 원리에 의해 기울기 소멸 현상이 해결되는지 알아야 한다.\n",
    "이러한 게이트들이 현재 타임스텝의 입력과 이전 타임스텝의 히든스테이트를 바탕으로\n",
    "열지 닫을지를 결정하는 것을 배움.\n",
    "그러한 게이트들이 복잡하게 얽혀서 결국 두 개(셀, 히든)의 스테이트가 각각 나오게 된다.\n",
    "그렇게 해서 LSTM은 각 타임 스텝별로 진행\n",
    "\n",
    "<Gated Recurrent Unit(GRU) [Cho et al., 2014]>\n",
    "LSTM은 게이트마다 웨이트 파라미터가 다 있어서 굉장히 많고 무겁다.\n",
    "그 단점을 보완하고자 하는 게 GRU\n",
    "LSTM보다 훨씬 가볍다라는 장점 있음\n",
    "동작 원리는 비슷\n",
    "\n",
    "<요약>\n",
    "-LSTM vs GRU?\n",
    "실제로는 LSTM이 좀 더 널리 쓰이는 추세.(딱히 이유 없음)\n",
    "-LSTM은 바닐라RNN에 비해서 훨씬 많은 파라미터를 가짐\n",
    "따라서 더 많은 학습 데이터와 학습 시간이 필요\n",
    "예전에는 LSTM이 무거워 컴퓨팅 파워 문제로 어려움이 있었지만\n",
    "이제는 바닐라 RNN 쓰는 경우가 드물다.\n",
    "-비록 LSTM이 그레디언트 배니싱 문제를 해결했지만,\n",
    "무조건 긴 데이터를 모두 기억할 수 있는 것은 아님\n",
    "\t-Network cpapcity의 한계는 존재함(한정된 벡터의 크기를 가지고 있기 때문에)\n",
    "\t몇백 타임스텝 할 수 있다거나 그렇지는 않다는 것\n",
    "\t-Attention을 통해 이를 해결할 수 있음\n",
    "\t한 문서를 다 읽혀보고 싶을 때…등\n",
    "\t어텐션이 NLP를 이끌어가고 있다고 해도 과언이 아님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 47. Ch 08. Recurrent Neural Networks - 07. Gradient Vanishing과 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How does LSTM solve Gradient Vanishing?\n",
    "<그레디언트 배니싱>\n",
    "RNN은 BPTT라는 알고리즘을 통해서 역전파를 수행한다\n",
    "BPTT는 피드포워드할 때 모든 관여된 연산들은 역전파가 동일하게 이뤄져야 한다는 것이고\n",
    "그 과정에서 프사이라고 하는 웨이트 파라미터가 타임스텝마다 사용이 됐는데\n",
    "그것들이 각각 역전파해서 모두 더하면 된다. \n",
    "문제는 액티베이션함수로 쓰인 tanh가 중첩된 상태로\n",
    "역전파 과정에서 이것이 계속해서 곱해지게 되면 기울기 소멸 현상이 발생\n",
    "뒤쪽에 있는 타임스텝의 프사이는 큰 영향을 받게 되지만\n",
    "앞쪽에 있는 타임스텝의 프사이는 굉장히 미미한 영향만 받게 됨.\n",
    "프사이가 바뀌지 않음\n",
    "이는 마치 Long range distance를 기억하지 못한다는 것과 같게 됨\n",
    "바닐라 RNN은 롱시퀀스를 잘 학습하지 못하고 앞쪽에 있는 것을 잘 기억하지 못한다는 문제가 생김\n",
    "\n",
    "<BPTT in LSTM>\n",
    "LSTM에서는 BPTT가 일어날 때 기울기 소멸 문제가 발생하지 않도록 하는 장치들이 있다\n",
    "LSTM은 히든 스테이트와, 셀 스테이트라는 또 하나의 히든 스테이트가 존재.\n",
    "셀 스테이트는 현재 자기 자신의 셀 스테이트를 만드는 데 있어서 지난 타임 스텝의 셀 스테이트가 들어감\n",
    "f를 어떻게 배우냐에 따라서 그레디언트를 뒤로 전달하거냐 말거냐가 정해진다\n",
    "\n",
    "<Still we have gradient vanishing in depth>\n",
    "LSTM solves only in time axis.\n",
    "레이어도 너무 깊게 쌓으면 기울기 소멸 문제 발생\n",
    "깊게 쌓고 싶으면 나중에 배울 residual connection 등을 이용해 깊게 쌓을 수 있다\n",
    "그러면 시간축 뿐만 아니라 뎁스에 대해서도 그레디언트 배니싱을 해결할 수 있다\n",
    "레이어는 4개 정도 권장. 초과해서 쌓으면 무의미한 것 같다\n",
    "구글에서 출판한 시퀀스투시퀀스 기계번역 같은 경우에는 4개까지는 괜찮고\n",
    "그보다 많이 쌓을 때는 residual connection을 쓰는 게 훨씬 낳다\n",
    "자기들도 그걸 사용해서 8개 층을 쌓았다라고 밝힘.\n",
    "트랜스포머 이전의 시퀀스투시퀀스 썼을 때 기계번역 얘기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : 48. Ch 08. Recurrent Neural Networks - 08. 실습 브리핑"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
