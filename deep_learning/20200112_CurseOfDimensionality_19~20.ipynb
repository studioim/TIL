{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Ch 04. Geometric Perspective - 01. 차원의 저주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curse of dimensionality\n",
    "차원이 높아짐에 따라 데이터가 희소하게 분포하게 되는 문제가 발생\n",
    "\n",
    "<Sparseness in High Dimesional Space>\n",
    "-d차원의 공간의 구(sphere) 안에 임의로 n개의 점을 흩뿌려보자.\n",
    "-이때, 구 테두리(회색 영역)와 안쪽에 위치한 점의 개수를 살펴보자.\n",
    "-차원이 증가함에 따른 각 영역 별 점의 개수의 차이는 어떻게 될까?\n",
    "차원이 높아질 수록 회색 영역이 점점 커지게 된다.\n",
    "1000차원 정도 되면 회색 영역의 비율이 훨씬 커짐.\n",
    "차원이 증가함에 따라 구 내부(흰색 영역)는 텅텅 비어감.\n",
    "\n",
    "<Curse of Dimensionality>\n",
    "-차원이 높을수록 데이터는 희소하게 분포하게 돼 학습이 어려워진다.\n",
    "차원이 높을수록 살펴봐야 할 공간이 많아진다. 빈 공간도 많아짐. \n",
    "모든 점들을 학습하기 위해서, 모든 구역들을 살펴봐야 함\n",
    "\n",
    "무조건 낮은 차원이 좋냐? 그건 또 아님.\n",
    "같은 구역 내의 점들은 서로 구별할 수 없음.\n",
    "샘플 포인트 만큼 차원의 개수가 있어도 비효율적.\n",
    "\n",
    "차원이 높아도 학습 어려워지고 낮아도 안 됨. 적절한 차원 존재\n",
    "\n",
    "-같은 정보의 데이터를 표현할 때, 차원이 높아질수록 희소성(sparseness)이 증가\n",
    "-희소성이 높을수록 모델링의 난이도가 높아짐\n",
    "가우시안 믹스쳐를 피팅하고자 할 때.\n",
    "케이민즈 클러스터링을 수행하고자 할 때(수백 차원에서는 잘 안 됨)\n",
    "-따라서 데이터의 특징(피처)을 더럽히지 않으면서 낮은 차원에서 표현해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Ch 04. Geometric Perspective - 02. 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<예) MNIST>\n",
    "MNIST: 28*28 = 784 dimension\n",
    "784차원의 공간\n",
    "구석 공간에는 데이터가 존재하지 않을 것. 픽셀이 흰색일 것\n",
    "이러한 정보들은 필요가 없고, 없애서 차원 축소할 여지가 생기는 것\n",
    "날려도 mnist 표현하는데 문제 없음\n",
    "\n",
    "<Linear Dimension Reduction: PCA>\n",
    "주성분분석\n",
    "-n차원의 공간에 샘플들의 분포가 주어져 있을 떄, 분포를 잘 설명하기 위한 새로운 axis를 찾아내는 과정\n",
    "-새로운 axis는 두 가지 조건을 만족해야 한다\n",
    "1)빨간 점 사이의 거리의 합이 최대가 되도록(분산이 최대가 되도록)\n",
    "2)검은 점과 검은 선 사이 거리의 합이 최소가 되도록(정보의 손실 최소화)\n",
    "-새롭게 찾아낸 액시스에 샘플들을 투사(프로젝션)하면 차원 축소가 가능\n",
    "2차원의 데이터를 1차원으로 압축. 그 과정에서 손실이 발생.\n",
    "손실은 검은 점이 투영되면서 움직인 거리.\n",
    "\n",
    "<Why we need dimension reduction?>\n",
    "Binary Classification in 2-D\n",
    "결정 경계가 선형인 경우\n",
    "투영 되는 선의 특정 지점 이하는 파란색, 이상은 초록색 등으로 구분할 수 있다.\n",
    "\n",
    "<Limitation of Linear Dimension Reduction>\n",
    "결정 경계가 비선형인 경우\n",
    "선형적 차원축소로는 결정 경계를 찾기 어렵다.\n",
    "이때 빛을 발하는 것이 딥뉴럴네트워크.\n",
    "오토인코더에서 봤듯이 차원축소 과정을 통해서 압축을 배운다고 했다.\n",
    "비선형적 차원축소를 말하는 거고 이를 통해 복잡한 결정경계도 찾을 수 있다는 것.\n",
    "고차원일수록 논리니어할 가능성이 높다.\n",
    "논리니어한 차원 축소를 실행하는 딥뉴럴 네트워크가 고차원 상의 데이터를 저차원으로 잘 축소할 수 있고 다른 알고리즘에 비해 문제를 잘 푼다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: 21. Ch 04. Geometric Perspective - 03. 매니폴드(Manifold) 가설"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
